{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Analytics mit DuckDB\n",
    "\n",
    "Dieses Notebook hilft dir bei der Analyse deiner Search Analytics Daten mit DuckDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# QUICK START\n",
    "\n",
    "**Nur diese 2 Zellen ausführen, um loszulegen!**\n",
    "\n",
    "### Schritt 1: Passe den Pfad zu deiner CSV-Datei an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#  EINZIGE EINSTELLUNG: Pfad zu deiner CSV-Datei               #\n",
    "#################################################################\n",
    "\n",
    "CSV_PATH = '../data/search_export.csv'    # <-- HIER ANPASSEN!\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Diese Zelle ausführen - alles andere passiert automatisch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== AUTOMATISCHES SETUP =====\n# Diese Zelle:\n# 1. Importiert alle benötigten Libraries\n# 2. Erstellt die Datenbank (.db Datei)\n# 3. Liest deine CSV und erstellt automatisch eine Tabelle\n# 4. Konvertiert deutsche Datumsformate (DD.MM.YYYY HH:MM) automatisch\n# 5. Zeigt dir was importiert wurde\n\nimport duckdb\nimport pandas as pd\nimport re\nfrom pathlib import Path\n\n# Plotting optional\ntry:\n    import matplotlib.pyplot as plt\n    import matplotlib.dates as mdates\n    plt.style.use('seaborn-v0_8-whitegrid')\n    PLOTTING_AVAILABLE = True\nexcept ImportError:\n    PLOTTING_AVAILABLE = False\n\n# Pandas Display-Optionen\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\n\n# Datenbank erstellen (im gleichen Ordner wie die CSV)\ncsv_path = Path(CSV_PATH)\ndb_path = csv_path.parent / 'searchanalytics.db'\n\n# Verbindung herstellen\ncon = duckdb.connect(str(db_path))\n\n# Hilfsfunktionen\ndef query(sql):\n    \"\"\"SQL ausführen und DataFrame zurückgeben\"\"\"\n    return con.execute(sql).df()\n\ndef execute(sql):\n    \"\"\"SQL ausführen ohne Rückgabe\"\"\"\n    con.execute(sql)\n\n# Tabelle erstellen aus CSV\nprint(\"=\"*60)\nprint(\"AUTOMATISCHER IMPORT\")\nprint(\"=\"*60)\n\n# Falls Tabelle bereits existiert, löschen und neu erstellen\nexecute(\"DROP TABLE IF EXISTS searches\")\n\n# CSV einlesen\nexecute(f\"\"\"\n    CREATE TABLE searches AS\n    SELECT * FROM read_csv('{CSV_PATH}', auto_detect=true)\n\"\"\")\n\n# Deutsche Datumsformate automatisch konvertieren (DD.MM.YYYY oder DD.MM.YYYY HH:MM)\nprint(\"\\nPrüfe auf deutsche Datumsformate...\")\nschema = query(\"DESCRIBE searches\")\nvarchar_cols = schema[schema['column_type'] == 'VARCHAR']['column_name'].tolist()\n\nconverted_cols = []\nfor col in varchar_cols:\n    # Ersten nicht-null Wert prüfen\n    sample = query(f\"SELECT {col} FROM searches WHERE {col} IS NOT NULL LIMIT 1\")\n    if len(sample) > 0:\n        val = str(sample.iloc[0, 0])\n        # Deutsches Datum erkennen: DD.MM.YYYY oder DD.MM.YYYY HH:MM\n        if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{4}', val):\n            try:\n                # Format bestimmen\n                if re.match(r'^\\d{2}\\.\\d{2}\\.\\d{4} \\d{2}:\\d{2}(:\\d{2})?$', val):\n                    # Mit Uhrzeit\n                    if val.count(':') == 2:\n                        fmt = '%d.%m.%Y %H:%M:%S'\n                    else:\n                        fmt = '%d.%m.%Y %H:%M'\n                else:\n                    # Nur Datum\n                    fmt = '%d.%m.%Y'\n                \n                # Konvertieren\n                execute(f\"ALTER TABLE searches ADD COLUMN {col}_temp TIMESTAMP\")\n                execute(f\"UPDATE searches SET {col}_temp = strptime({col}, '{fmt}')\")\n                execute(f\"ALTER TABLE searches DROP COLUMN {col}\")\n                execute(f\"ALTER TABLE searches RENAME COLUMN {col}_temp TO {col}\")\n                converted_cols.append(col)\n            except Exception as e:\n                print(f\"  Warnung: Konnte {col} nicht konvertieren: {e}\")\n\nif converted_cols:\n    print(f\"  Konvertiert: {', '.join(converted_cols)}\")\nelse:\n    print(\"  Keine deutschen Datumsformate gefunden\")\n\n# Was wurde importiert?\nrow_count = query(\"SELECT COUNT(*) as n FROM searches\")['n'][0]\nprint(f\"\\n CSV-Datei: {csv_path.name}\")\nprint(f\" Datenbank: {db_path.name}\")\nprint(f\" Importiert: {row_count:,} Zeilen\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ERKANNTE SPALTEN\")\nprint(\"=\"*60)\nschema = query(\"DESCRIBE searches\")\nfor _, row in schema.iterrows():\n    print(f\"  {row['column_name']:30} {row['column_type']}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ERSTE 5 ZEILEN\")\nprint(\"=\"*60)\ndisplay(query(\"SELECT * FROM searches LIMIT 5\"))\n\nprint(\"\\n Setup abgeschlossen! Du kannst jetzt die Analyse-Zellen unten ausführen.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ANALYSEN\n",
    "\n",
    "Ab hier kannst du die Zellen ausführen, die dich interessieren.\n",
    "\n",
    "**Wichtig:** Falls deine Spalten anders heißen, passe die Namen in den Queries an!\n",
    "Typische Spalten-Varianten:\n",
    "- Zeitstempel: `timestamp`, `date`, `datetime`, `created_at`\n",
    "- Suchbegriff: `search_query`, `query`, `search_term`, `keyword`\n",
    "- Ergebnisse: `results_count`, `result_count`, `hits`, `total_results`\n",
    "- Antwortzeit: `response_time`, `duration`, `latency_ms`\n",
    "\n",
    "---\n",
    "## Basis-Statistiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersicht: Was haben wir?\n",
    "query(\"DESCRIBE searches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste und letzte Einträge\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_rows,\n",
    "        MIN(timestamp) as first_entry,\n",
    "        MAX(timestamp) as last_entry\n",
    "    FROM searches\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispieldaten anschauen\n",
    "query(\"SELECT * FROM searches LIMIT 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Zeitliche Verteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einträge pro Tag\n",
    "# HINWEIS: Ersetze 'timestamp' durch deine Datumsspalte falls nötig\n",
    "\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        DATE_TRUNC('day', timestamp)::DATE as datum,\n",
    "        COUNT(*) as anzahl\n",
    "    FROM searches\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 30\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung nach Stunde\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(HOUR FROM timestamp) as stunde,\n",
    "        COUNT(*) as anzahl,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 2) as prozent\n",
    "    FROM searches\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung nach Wochentag\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        DAYNAME(timestamp) as wochentag,\n",
    "        DAYOFWEEK(timestamp) as tag_nr,\n",
    "        COUNT(*) as anzahl\n",
    "    FROM searches\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Top Werte (für jede Spalte anpassbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 häufigste Werte einer Spalte\n",
    "# HINWEIS: Ersetze 'search_query' durch die Spalte die dich interessiert\n",
    "\n",
    "SPALTE = 'search_query'  # <-- Hier anpassen\n",
    "\n",
    "query(f\"\"\"\n",
    "    SELECT\n",
    "        {SPALTE},\n",
    "        COUNT(*) as anzahl,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 2) as prozent\n",
    "    FROM searches\n",
    "    WHERE {SPALTE} IS NOT NULL\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 20\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Null-Ergebnis-Analyse\n",
    "\n",
    "Falls du eine Spalte mit Ergebnis-Anzahl hast (z.B. `results_count`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamt Null-Rate\n",
    "# HINWEIS: Ersetze 'results_count' durch deine Spalte\n",
    "\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total_searches,\n",
    "        SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) as null_results,\n",
    "        ROUND(100.0 * SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) / COUNT(*), 2) as null_rate_pct\n",
    "    FROM searches\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suchbegriffe mit den meisten Null-Ergebnissen\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        search_query,\n",
    "        COUNT(*) as anzahl,\n",
    "        SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) as null_results,\n",
    "        ROUND(100.0 * SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as null_rate_pct\n",
    "    FROM searches\n",
    "    WHERE search_query IS NOT NULL\n",
    "    GROUP BY 1\n",
    "    HAVING COUNT(*) >= 5  -- Mindestens 5 Suchen\n",
    "    ORDER BY null_results DESC\n",
    "    LIMIT 20\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null-Rate pro Tag\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        DATE_TRUNC('day', timestamp)::DATE as datum,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) as null_results,\n",
    "        ROUND(100.0 * SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) / COUNT(*), 2) as null_rate_pct\n",
    "    FROM searches\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1 DESC\n",
    "    LIMIT 30\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Performance-Metriken\n",
    "\n",
    "Falls du eine Spalte mit Antwortzeit hast (z.B. `response_time`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Time Statistiken\n",
    "# HINWEIS: Ersetze 'response_time' durch deine Spalte\n",
    "\n",
    "query(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) as total,\n",
    "        ROUND(AVG(response_time), 2) as avg_ms,\n",
    "        ROUND(MEDIAN(response_time), 2) as median_ms,\n",
    "        ROUND(PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY response_time), 2) as p90_ms,\n",
    "        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time), 2) as p95_ms,\n",
    "        ROUND(PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_time), 2) as p99_ms,\n",
    "        ROUND(MAX(response_time), 2) as max_ms\n",
    "    FROM searches\n",
    "    WHERE response_time IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualisierungen\n",
    "\n",
    "Falls Matplotlib installiert ist (`conda install matplotlib`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTTING_AVAILABLE:\n",
    "    # Einträge pro Tag\n",
    "    daily = query(\"\"\"\n",
    "        SELECT\n",
    "            DATE_TRUNC('day', timestamp)::DATE as datum,\n",
    "            COUNT(*) as anzahl\n",
    "        FROM searches\n",
    "        GROUP BY 1\n",
    "        ORDER BY 1\n",
    "    \"\"\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    ax.plot(daily['datum'], daily['anzahl'], linewidth=2, color='steelblue')\n",
    "    ax.fill_between(daily['datum'], daily['anzahl'], alpha=0.3, color='steelblue')\n",
    "    ax.set_title('Einträge pro Tag', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Datum')\n",
    "    ax.set_ylabel('Anzahl')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Matplotlib nicht installiert. Führe aus: conda install matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTTING_AVAILABLE:\n",
    "    # Verteilung nach Stunde\n",
    "    hourly = query(\"\"\"\n",
    "        SELECT\n",
    "            EXTRACT(HOUR FROM timestamp)::INT as stunde,\n",
    "            COUNT(*) as anzahl\n",
    "        FROM searches\n",
    "        GROUP BY 1\n",
    "        ORDER BY 1\n",
    "    \"\"\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.bar(hourly['stunde'], hourly['anzahl'], color='steelblue')\n",
    "    ax.set_title('Verteilung nach Tageszeit', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Stunde')\n",
    "    ax.set_ylabel('Anzahl')\n",
    "    ax.set_xticks(range(0, 24))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOTTING_AVAILABLE:\n",
    "    # Top 10 Werte\n",
    "    SPALTE = 'search_query'  # <-- Anpassen\n",
    "    \n",
    "    top = query(f\"\"\"\n",
    "        SELECT {SPALTE} as wert, COUNT(*) as anzahl\n",
    "        FROM searches\n",
    "        WHERE {SPALTE} IS NOT NULL AND {SPALTE} != ''\n",
    "        GROUP BY 1\n",
    "        ORDER BY 2 DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    y_pos = range(len(top))\n",
    "    ax.barh(y_pos, top['anzahl'], color='steelblue')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(top['wert'])\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f'Top 10: {SPALTE}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Anzahl')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Eigene Queries\n",
    "\n",
    "Hier kannst du eigene SQL-Queries schreiben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine eigene Query hier:\n",
    "query(\"\"\"\n",
    "    SELECT *\n",
    "    FROM searches\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch eine Query:\n",
    "query(\"\"\"\n",
    "    SELECT *\n",
    "    FROM searches\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Export für Power BI\n\nParquet-Dateien sind ideal für Power BI: kleiner, schneller, Datentypen bleiben erhalten."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rohdaten Export (alle Daten)\nfrom datetime import datetime\n\noutput_file = f'../output/searches_raw_{datetime.now().strftime(\"%Y%m%d\")}.parquet'\nexecute(f\"COPY searches TO '{output_file}' (FORMAT PARQUET)\")\n\n# Dateigröße anzeigen\nimport os\nsize_mb = os.path.getsize(output_file) / (1024 * 1024)\nrow_count = query(\"SELECT COUNT(*) as n FROM searches\")['n'][0]\n\nprint(f\"Exportiert: {output_file}\")\nprint(f\"Zeilen:     {row_count:,}\")\nprint(f\"Größe:      {size_mb:.1f} MB\")\nprint(f\"\\nIn Power BI: Get Data → Parquet\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregierte Tagesdaten (für lange Zeiträume / Trend-Dashboards)\noutput_file = f'../output/searches_daily_{datetime.now().strftime(\"%Y%m%d\")}.parquet'\n\nexecute(f\"\"\"\n    COPY (\n        SELECT \n            DATE_TRUNC('day', timestamp)::DATE as datum,\n            COUNT(*) as total_searches,\n            COUNT(DISTINCT search_query) as unique_queries,\n            SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) as null_results,\n            ROUND(100.0 * SUM(CASE WHEN results_count = 0 THEN 1 ELSE 0 END) / COUNT(*), 2) as null_rate_pct\n        FROM searches\n        GROUP BY 1\n        ORDER BY 1\n    ) TO '{output_file}' (FORMAT PARQUET)\n\"\"\")\n\nsize_mb = os.path.getsize(output_file) / (1024 * 1024)\ndays = query(\"SELECT COUNT(DISTINCT DATE_TRUNC('day', timestamp)) as n FROM searches\")['n'][0]\n\nprint(f\"Exportiert: {output_file}\")\nprint(f\"Tage:       {days}\")\nprint(f\"Größe:      {size_mb:.2f} MB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neue Daten hinzufügen\n",
    "\n",
    "Falls du später weitere CSV-Dateien importieren möchtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue CSV an bestehende Tabelle anhängen\n",
    "# NEW_CSV = '../data/neue_daten.csv'\n",
    "\n",
    "# execute(f\"\"\"\n",
    "#     INSERT INTO searches\n",
    "#     SELECT * FROM read_csv('{NEW_CSV}', auto_detect=true)\n",
    "# \"\"\")\n",
    "\n",
    "# print(f\"Neue Daten hinzugefügt. Gesamt: {query('SELECT COUNT(*) FROM searches')['count_star()'][0]:,} Zeilen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Aufräumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbindung schließen (am Ende der Session)\n",
    "con.close()\n",
    "print(\"Verbindung geschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notizen\n",
    "\n",
    "**Meine Spalten:**\n",
    "- ...\n",
    "\n",
    "**Erkenntnisse:**\n",
    "- ...\n",
    "\n",
    "**Offene Fragen:**\n",
    "- ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}