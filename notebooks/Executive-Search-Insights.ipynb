{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Search Insights\n",
    "## Intranet Search Analytics -- Interview Reference\n",
    "\n",
    "This notebook extracts key metrics from the intranet search analytics database and presents them as **executive-level findings**, organized for use with the **STAR interview framework**.\n",
    "\n",
    "**Prerequisites:** The DuckDB database must exist at `../data/searchanalytics.db`. Run `python process_search_analytics.py` first.\n",
    "\n",
    "**Audience:** Internal Communications Executive\n",
    "\n",
    "**Sections:**\n",
    "1. Executive Summary Dashboard\n",
    "2. Search Volume & Adoption Trends\n",
    "3. Search Quality Scorecard\n",
    "4. Content Gap Analysis (The Big Win)\n",
    "5. Search Term Performance\n",
    "6. User Journey & Behavior Insights\n",
    "7. Regional & Temporal Patterns\n",
    "8. Interview Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SETUP & DATABASE CONNECTION =====\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Color scheme\n",
    "C = {\n",
    "    'success': '#2E7D32', 'success_light': '#81C784',\n",
    "    'fail': '#C62828', 'fail_light': '#EF9A9A',\n",
    "    'warn': '#F57C00', 'warn_light': '#FFB74D',\n",
    "    'neutral': '#1565C0', 'neutral_light': '#64B5F6',\n",
    "    'gray': '#757575', 'gray_light': '#E0E0E0',\n",
    "}\n",
    "OUTCOME_COLORS = {\n",
    "    'Success': C['success'], 'Engaged': C['neutral'],\n",
    "    'Abandoned': C['warn'], 'No Results': C['fail'], 'Unknown': C['gray'],\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.1f}')\n",
    "\n",
    "# Connect\n",
    "db_path = Path('../data/searchanalytics.db')\n",
    "if not db_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Database not found at {db_path}\\n\"\n",
    "        \"Run: python process_search_analytics.py\")\n",
    "\n",
    "con = duckdb.connect(str(db_path), read_only=True)\n",
    "\n",
    "def query(sql):\n",
    "    \"\"\"Execute SQL and return DataFrame\"\"\"\n",
    "    return con.execute(sql).df()\n",
    "\n",
    "def fmt_pct(val, decimals=1):\n",
    "    if pd.isna(val): return 'N/A'\n",
    "    return f\"{val:.{decimals}f}%\"\n",
    "\n",
    "def fmt_num(val):\n",
    "    if pd.isna(val): return 'N/A'\n",
    "    return f\"{int(val):,}\"\n",
    "\n",
    "# Verify tables\n",
    "tables = query(\"SHOW TABLES\")['name'].tolist()\n",
    "required = ['searches', 'searches_daily', 'searches_journeys', 'searches_terms']\n",
    "missing = [t for t in required if t not in tables]\n",
    "if missing:\n",
    "    print(f\"WARNING: Missing tables: {missing}\")\n",
    "    print(\"Run: python process_search_analytics.py --full-refresh\")\n",
    "else:\n",
    "    for t in required:\n",
    "        n = query(f\"SELECT COUNT(*) as n FROM {t}\")['n'][0]\n",
    "        print(f\"  {t}: {n:,} rows\")\n",
    "\n",
    "date_range = query(\"\"\"\n",
    "    SELECT MIN(session_date) as first_date, MAX(session_date) as last_date,\n",
    "           COUNT(DISTINCT session_date) as days\n",
    "    FROM searches\n",
    "\"\"\")\n",
    "print(f\"\\nDate range: {date_range['first_date'][0]} to {date_range['last_date'][0]} ({date_range['days'][0]} days)\")\n",
    "print(\"\\nReady.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Executive Summary Dashboard\n",
    "**Business Question:** What is the overall health of our intranet search, at a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXECUTIVE SUMMARY KPIs =====\n",
    "summary = query(\"\"\"\n",
    "    SELECT\n",
    "        MIN(date) as date_from,\n",
    "        MAX(date) as date_to,\n",
    "        COUNT(*) as days_covered,\n",
    "        SUM(search_starts) as total_searches,\n",
    "        SUM(unique_users) as total_user_days,\n",
    "        SUM(unique_sessions) as total_sessions,\n",
    "        SUM(unique_search_terms) as total_unique_terms,\n",
    "        -- Session Success Rate % = sessions_with_clicks / sessions_with_results\n",
    "        ROUND(100.0 * SUM(sessions_with_clicks) / NULLIF(SUM(sessions_with_results), 0), 1) as session_success_rate,\n",
    "        -- Null Result Rate % = null_results / result_events\n",
    "        ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as null_result_rate,\n",
    "        -- Abandonment Rate % = sessions_abandoned / sessions_with_results\n",
    "        ROUND(100.0 * SUM(sessions_abandoned) / NULLIF(SUM(sessions_with_results), 0), 1) as abandonment_rate,\n",
    "        -- Avg Searches per Session\n",
    "        ROUND(SUM(search_starts) * 1.0 / NULLIF(SUM(unique_sessions), 0), 2) as avg_searches_per_session,\n",
    "        -- Reformulation rate from journeys\n",
    "        ROUND(100.0 * (SELECT COUNT(*) FROM searches_journeys WHERE had_reformulation = true)\n",
    "            / NULLIF((SELECT COUNT(*) FROM searches_journeys), 0), 1) as reformulation_rate\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "s = summary.iloc[0]\n",
    "print(\"=\" * 64)\n",
    "print(\"  INTRANET SEARCH ANALYTICS -- EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 64)\n",
    "print(f\"  Period:     {s['date_from']} to {s['date_to']} ({int(s['days_covered'])} days)\")\n",
    "print(f\"  Searches:   {fmt_num(s['total_searches'])}\")\n",
    "print(f\"  Users:      {fmt_num(s['total_user_days'])} user-days\")\n",
    "print(f\"  Sessions:   {fmt_num(s['total_sessions'])}\")\n",
    "print(\"-\" * 64)\n",
    "print(f\"  Session Success Rate:    {fmt_pct(s['session_success_rate'])}\")\n",
    "print(f\"  Null Result Rate:        {fmt_pct(s['null_result_rate'])}\")\n",
    "print(f\"  Abandonment Rate:        {fmt_pct(s['abandonment_rate'])}\")\n",
    "print(f\"  Avg Searches/Session:    {s['avg_searches_per_session']:.2f}\")\n",
    "print(f\"  Reformulation Rate:      {fmt_pct(s['reformulation_rate'])}\")\n",
    "print(\"=\" * 64)\n",
    "\n",
    "# Store for cheat sheet\n",
    "EXEC = s.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I built a search analytics solution tracking **[total_searches]** searches across **[users]** users over **[days]** days. The key finding: our session success rate was **[X]%**, meaning roughly **[Y in Z]** search sessions resulted in users clicking on actual content. The null result rate of **[X]%** represented our biggest content gap opportunity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Search Volume & Adoption Trends\n",
    "**Business Question:** Is intranet search usage growing? How are users adopting the platform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SEARCH VOLUME OVER TIME =====\n",
    "daily = query(\"\"\"\n",
    "    SELECT date, search_starts, unique_users, unique_sessions,\n",
    "           day_of_week, day_of_week_num, new_users, returning_users\n",
    "    FROM searches_daily\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "ax1.bar(daily['date'], daily['search_starts'], color=C['neutral_light'],\n",
    "        alpha=0.7, label='Daily Searches', width=0.8)\n",
    "ax1.set_ylabel('Search Count', color=C['neutral'])\n",
    "ax1.tick_params(axis='y', labelcolor=C['neutral'])\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "ax1.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(daily['date'], daily['unique_users'], color=C['success'],\n",
    "         linewidth=2, marker='o', markersize=3, label='Unique Users')\n",
    "ax2.set_ylabel('Unique Users', color=C['success'])\n",
    "ax2.tick_params(axis='y', labelcolor=C['success'])\n",
    "\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.12, 0.95))\n",
    "plt.title('Daily Search Volume & Unique Users')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Growth rate\n",
    "if len(daily) >= 14:\n",
    "    first_week = daily.head(7)['search_starts'].sum()\n",
    "    last_week = daily.tail(7)['search_starts'].sum()\n",
    "    growth = ((last_week - first_week) / first_week * 100) if first_week > 0 else 0\n",
    "    print(f\"Growth (first week vs last week): {growth:+.1f}%\")\n",
    "    print(f\"  First 7 days: {first_week:,} searches\")\n",
    "    print(f\"  Last 7 days:  {last_week:,} searches\")\n",
    "\n",
    "avg_daily = daily['search_starts'].mean()\n",
    "avg_users = daily['unique_users'].mean()\n",
    "print(f\"\\nAverage daily searches: {avg_daily:,.0f}\")\n",
    "print(f\"Average daily unique users: {avg_users:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== NEW vs RETURNING USERS =====\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.fill_between(daily['date'], 0, daily['returning_users'],\n",
    "                color=C['neutral_light'], alpha=0.7, label='Returning Users')\n",
    "ax.fill_between(daily['date'], daily['returning_users'],\n",
    "                daily['returning_users'] + daily['new_users'],\n",
    "                color=C['success_light'], alpha=0.7, label='New Users')\n",
    "ax.set_ylabel('Users')\n",
    "ax.set_title('New vs Returning Users Over Time')\n",
    "ax.legend(loc='upper left')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_new = daily['new_users'].sum()\n",
    "total_returning = daily['returning_users'].sum()\n",
    "total_all = total_new + total_returning\n",
    "if total_all > 0:\n",
    "    print(f\"New users:       {total_new:,} ({100*total_new/total_all:.1f}%)\")\n",
    "    print(f\"Returning users: {total_returning:,} ({100*total_returning/total_all:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DAY OF WEEK PATTERNS =====\n",
    "dow = daily.groupby(['day_of_week', 'day_of_week_num']).agg(\n",
    "    avg_searches=('search_starts', 'mean'),\n",
    "    total_searches=('search_starts', 'sum'),\n",
    "    avg_users=('unique_users', 'mean'),\n",
    ").reset_index().sort_values('day_of_week_num')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(dow['day_of_week'], dow['avg_searches'], color=C['neutral'])\n",
    "for i, (_, row) in enumerate(dow.iterrows()):\n",
    "    if row['day_of_week_num'] >= 6:\n",
    "        bars[i].set_color(C['gray_light'])\n",
    "ax.set_ylabel('Average Daily Searches')\n",
    "ax.set_title('Search Volume by Day of Week')\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{bar.get_height():.0f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "busiest = dow.loc[dow['avg_searches'].idxmax()]\n",
    "quietest = dow.loc[dow['avg_searches'].idxmin()]\n",
    "print(f\"Busiest day:  {busiest['day_of_week']} ({busiest['avg_searches']:.0f} avg searches)\")\n",
    "print(f\"Quietest day: {quietest['day_of_week']} ({quietest['avg_searches']:.0f} avg searches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Search adoption showed **[growth trend]** over the measurement period. We tracked **[X]** unique user-days, with **[Y]%** being returning users -- demonstrating strong repeat engagement. **[Busiest day]** was consistently the peak day with **[N]** average daily searches, while weekends showed minimal activity, confirming this is a workplace productivity tool.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Search Quality Scorecard\n",
    "**Business Question:** Are users finding what they need? Is search quality improving over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUALITY METRICS TRENDED =====\n",
    "quality = query(\"\"\"\n",
    "    SELECT date, sessions_with_clicks, sessions_with_results, sessions_abandoned,\n",
    "           null_results, result_events, search_starts, success_clicks, unique_sessions\n",
    "    FROM searches_daily\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "quality['date'] = pd.to_datetime(quality['date'])\n",
    "\n",
    "# Daily rates\n",
    "quality['session_success_rate'] = 100.0 * quality['sessions_with_clicks'] / quality['sessions_with_results'].replace(0, np.nan)\n",
    "quality['null_result_rate'] = 100.0 * quality['null_results'] / quality['result_events'].replace(0, np.nan)\n",
    "quality['abandonment_rate'] = 100.0 * quality['sessions_abandoned'] / quality['sessions_with_results'].replace(0, np.nan)\n",
    "\n",
    "# 7-day rolling averages\n",
    "for col in ['session_success_rate', 'null_result_rate', 'abandonment_rate']:\n",
    "    quality[f'{col}_7d'] = quality[col].rolling(7, min_periods=1).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(quality['date'], quality['session_success_rate_7d'],\n",
    "        color=C['success'], linewidth=2.5, label='Session Success Rate %')\n",
    "ax.plot(quality['date'], quality['null_result_rate_7d'],\n",
    "        color=C['fail'], linewidth=2.5, label='Null Result Rate %')\n",
    "ax.plot(quality['date'], quality['abandonment_rate_7d'],\n",
    "        color=C['warn'], linewidth=2.5, label='Abandonment Rate %')\n",
    "\n",
    "ax.set_ylabel('Rate (%)')\n",
    "ax.set_title('Search Quality Metrics -- 7-Day Rolling Average')\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall assessment\n",
    "overall_success = 100.0 * quality['sessions_with_clicks'].sum() / quality['sessions_with_results'].sum()\n",
    "overall_null = 100.0 * quality['null_results'].sum() / quality['result_events'].sum()\n",
    "overall_abandon = 100.0 * quality['sessions_abandoned'].sum() / quality['sessions_with_results'].sum()\n",
    "\n",
    "assessment = pd.DataFrame({\n",
    "    'Metric': ['Session Success Rate', 'Null Result Rate', 'Abandonment Rate'],\n",
    "    'Value': [f'{overall_success:.1f}%', f'{overall_null:.1f}%', f'{overall_abandon:.1f}%'],\n",
    "    'Target': ['>40%', '<5%', '<50%'],\n",
    "    'Assessment': [\n",
    "        'Good' if overall_success > 40 else ('Fair' if overall_success > 25 else 'Needs Improvement'),\n",
    "        'Good' if overall_null < 5 else ('Fair' if overall_null < 15 else 'Needs Improvement'),\n",
    "        'Good' if overall_abandon < 50 else ('Fair' if overall_abandon < 70 else 'Needs Improvement'),\n",
    "    ]\n",
    "})\n",
    "print(\"\\nOverall Quality Assessment:\")\n",
    "display(assessment.style.hide(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I established a search quality scorecard with three headline metrics. Session Success Rate was **[X]%** (target >40%), Null Result Rate was **[Y]%** (target <5%), and Abandonment Rate was **[Z]%** (target <50%). The 7-day rolling average showed **[improving/stable/declining]** quality over time, providing a clear signal for whether content and search improvements were working.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Content Gap Analysis (The Big Win)\n",
    "**Business Question:** What are employees searching for that we have no content for? Where is the biggest opportunity for Internal Communications?\n",
    "\n",
    "This is the most actionable section. Each zero-result term represents a **content gap** -- employees actively looking for information that does not exist on the intranet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOP ZERO-RESULT SEARCH TERMS =====\n",
    "zero_results = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term,\n",
    "        SUM(search_count) as total_searches,\n",
    "        SUM(null_result_count) as null_count,\n",
    "        SUM(result_events) as result_events,\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as null_rate,\n",
    "        SUM(unique_users) as unique_users_searching\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(null_result_count) > 0\n",
    "       AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) >= 50\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display_df = zero_results[['search_term', 'total_searches', 'null_count', 'null_rate', 'unique_users_searching']].rename(columns={\n",
    "    'search_term': 'Search Term',\n",
    "    'total_searches': 'Total Searches',\n",
    "    'null_count': 'Zero-Result Count',\n",
    "    'null_rate': 'Null Rate %',\n",
    "    'unique_users_searching': 'Unique Users',\n",
    "})\n",
    "\n",
    "print(\"TOP 20 CONTENT GAPS: Search Terms with >= 50% Null Rate\")\n",
    "print(\"Each row = employees searching for content that DOES NOT EXIST.\\n\")\n",
    "display(display_df.style.hide(axis='index')\n",
    "    .format({'Total Searches': '{:,}', 'Zero-Result Count': '{:,}',\n",
    "             'Null Rate %': '{:.1f}%', 'Unique Users': '{:,}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FAILED SEARCH VOLUME SUMMARY =====\n",
    "gap_summary = query(\"\"\"\n",
    "    SELECT\n",
    "        SUM(null_results) as total_failed,\n",
    "        SUM(result_events) as total_results,\n",
    "        ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as overall_null_rate\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "pure_zero = query(\"\"\"\n",
    "    SELECT COUNT(*) as cnt FROM (\n",
    "        SELECT search_term\n",
    "        FROM searches_terms\n",
    "        GROUP BY search_term\n",
    "        HAVING SUM(null_result_count) = SUM(result_events) AND SUM(result_events) > 0\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Volume covered by top 10 gaps\n",
    "top10_gap_volume = zero_results.head(10)['total_searches'].sum() if len(zero_results) >= 10 else zero_results['total_searches'].sum()\n",
    "all_gap_volume = query(\"\"\"\n",
    "    SELECT SUM(search_count) as vol FROM (\n",
    "        SELECT search_term, SUM(search_count) as search_count, SUM(null_result_count) as nulls, SUM(result_events) as results\n",
    "        FROM searches_terms GROUP BY search_term\n",
    "        HAVING SUM(null_result_count) > 0 AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) >= 50\n",
    "    )\n",
    "\"\"\")['vol'][0]\n",
    "top10_pct = 100.0 * top10_gap_volume / all_gap_volume if all_gap_volume > 0 else 0\n",
    "\n",
    "gs = gap_summary.iloc[0]\n",
    "print(f\"Total failed searches (zero results):      {fmt_num(gs['total_failed'])}\")\n",
    "print(f\"Overall null result rate:                   {fmt_pct(gs['overall_null_rate'])}\")\n",
    "print(f\"Unique terms that ALWAYS return zero:       {fmt_num(pure_zero['cnt'][0])}\")\n",
    "print(f\"Top 10 gap terms cover:                     {top10_pct:.0f}% of all gap-related searches\")\n",
    "\n",
    "# Chart: horizontal bar of top content gaps\n",
    "if len(zero_results) > 0:\n",
    "    top_n = zero_results.head(15).sort_values('total_searches')\n",
    "    fig, ax = plt.subplots(figsize=(12, max(5, len(top_n) * 0.45)))\n",
    "    ax.barh(top_n['search_term'], top_n['total_searches'], color=C['fail'])\n",
    "    ax.set_xlabel('Total Searches (returning zero results)')\n",
    "    ax.set_title('Top Content Gaps: Most-Searched Terms with Zero Results')\n",
    "    for i, (_, row) in enumerate(top_n.iterrows()):\n",
    "        ax.text(row['total_searches'] + 0.3, i, f\"{int(row['total_searches']):,}\",\n",
    "                va='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready) -- THE BIG WIN\n",
    "\n",
    "> **SITUATION:** Employees were searching the intranet but Internal Communications had no data on what content was missing.\n",
    ">\n",
    "> **TASK:** I built a search analytics pipeline to identify content gaps and measure search quality.\n",
    ">\n",
    "> **ACTION:** I analyzed all search terms that consistently returned zero results. I identified **[N]** unique terms that always fail and ranked them by search volume so the content team could prioritize.\n",
    ">\n",
    "> **RESULT:** The top content gap, \"[top term]\", was searched **[X]** times with zero results. Addressing just the top 10 terms would cover **[Y]%** of all gap-related search volume. This gave Internal Comms their first ever **data-driven content roadmap**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Search Term Performance\n",
    "**Business Question:** Which search terms are working well? Which need attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOP 20 TERMS BY VOLUME WITH SUCCESS RATES =====\n",
    "top_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        SUM(success_click_count) as \"Success Clicks\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        SUM(null_result_count) as \"Null Results\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\",\n",
    "        SUM(unique_users) as \"Unique Users\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 3\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 20 Search Terms by Volume\")\n",
    "display(top_terms.style.hide(axis='index')\n",
    "    .format({'Searches': '{:,}', 'Success Clicks': '{:,}',\n",
    "             'Success CTR %': '{:.1f}%', 'Null Results': '{:,}',\n",
    "             'Null Rate %': '{:.1f}%', 'Unique Users': '{:,}'})\n",
    "    .bar(subset=['Success CTR %'], color=C['success_light'], vmin=0, vmax=100)\n",
    "    .bar(subset=['Null Rate %'], color=C['fail_light'], vmin=0, vmax=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROBLEM TERMS vs SUCCESS STORIES =====\n",
    "\n",
    "# Problem: results exist but nobody clicks (relevance issue)\n",
    "problem_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 5\n",
    "       AND ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) < 20\n",
    "       AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) < 50\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(\"RELEVANCE PROBLEMS: Results shown but users don't click (CTR < 20%, Null Rate < 50%)\")\n",
    "print(\"Action: Improve content titles, metadata, or search ranking\\n\")\n",
    "if len(problem_terms) > 0:\n",
    "    display(problem_terms.style.hide(axis='index')\n",
    "        .format({'Searches': '{:,}', 'Success CTR %': '{:.1f}%', 'Null Rate %': '{:.1f}%'}))\n",
    "else:\n",
    "    print(\"No terms matching these criteria (good news!)\")\n",
    "\n",
    "# Success stories: high volume + high CTR\n",
    "success_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 5\n",
    "       AND ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) >= 30\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSUCCESS STORIES: High click-through terms (CTR >= 30%)\")\n",
    "print(\"These demonstrate what 'good' looks like -- benchmark for other content\\n\")\n",
    "if len(success_terms) > 0:\n",
    "    display(success_terms.style.hide(axis='index')\n",
    "        .format({'Searches': '{:,}', 'Success CTR %': '{:.1f}%', 'Null Rate %': '{:.1f}%'}))\n",
    "else:\n",
    "    print(\"No terms matching these criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUERY COMPLEXITY: Word count vs success =====\n",
    "complexity = query(\"\"\"\n",
    "    SELECT\n",
    "        CASE\n",
    "            WHEN word_count = 1 THEN '1 word'\n",
    "            WHEN word_count = 2 THEN '2 words'\n",
    "            WHEN word_count = 3 THEN '3 words'\n",
    "            WHEN word_count >= 4 THEN '4+ words'\n",
    "        END as query_length,\n",
    "        CASE\n",
    "            WHEN word_count = 1 THEN 1\n",
    "            WHEN word_count = 2 THEN 2\n",
    "            WHEN word_count = 3 THEN 3\n",
    "            WHEN word_count >= 4 THEN 4\n",
    "        END as sort_order,\n",
    "        SUM(search_count) as searches,\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as success_ctr,\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as null_rate,\n",
    "        ROUND(100.0 * SUM(search_count) / NULLIF((SELECT SUM(search_count) FROM searches_terms), 0), 1) as pct_of_total\n",
    "    FROM searches_terms\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 2\n",
    "\"\"\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Volume distribution\n",
    "axes[0].bar(complexity['query_length'], complexity['pct_of_total'], color=C['neutral'])\n",
    "axes[0].set_ylabel('% of Total Searches')\n",
    "axes[0].set_title('Query Length Distribution')\n",
    "for i, v in enumerate(complexity['pct_of_total']):\n",
    "    axes[0].text(i, v + 0.3, f'{v:.0f}%', ha='center', fontsize=10)\n",
    "\n",
    "# CTR by length\n",
    "axes[1].bar(complexity['query_length'], complexity['success_ctr'], color=C['success'])\n",
    "axes[1].set_ylabel('Success CTR %')\n",
    "axes[1].set_title('Success Rate by Query Length')\n",
    "for i, v in enumerate(complexity['success_ctr']):\n",
    "    axes[1].text(i, v + 0.3, f'{v:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "# Null rate by length\n",
    "axes[2].bar(complexity['query_length'], complexity['null_rate'], color=C['fail'])\n",
    "axes[2].set_ylabel('Null Result Rate %')\n",
    "axes[2].set_title('Null Rate by Query Length')\n",
    "for i, v in enumerate(complexity['null_rate']):\n",
    "    axes[2].text(i, v + 0.3, f'{v:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_length = complexity.loc[complexity['success_ctr'].idxmax()]\n",
    "print(f\"Best performing query length: {best_length['query_length']} (CTR: {best_length['success_ctr']:.1f}%)\")\n",
    "most_common = complexity.loc[complexity['searches'].idxmax()]\n",
    "print(f\"Most common query length: {most_common['query_length']} ({most_common['pct_of_total']:.0f}% of all searches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I categorized search terms into three groups: **Success Stories** (CTR > 30%), **Relevance Problems** (results exist but CTR < 20%), and **Content Gaps** (zero results). This framework gave Internal Comms a clear action plan for each category: create content for gaps, improve metadata for relevance problems, and replicate what works from success stories. **[Best length]**-word queries performed best at **[X]% CTR**, suggesting users get better results with more specific queries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. User Journey & Behavior Insights\n",
    "**Business Question:** How do search sessions play out? Where do users struggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== JOURNEY OUTCOME DISTRIBUTION =====\n",
    "outcomes = query(\"\"\"\n",
    "    SELECT\n",
    "        journey_outcome,\n",
    "        journey_outcome_sort,\n",
    "        COUNT(*) as sessions,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct\n",
    "    FROM searches_journeys\n",
    "    GROUP BY journey_outcome, journey_outcome_sort\n",
    "    ORDER BY journey_outcome_sort\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Donut chart\n",
    "colors_list = [OUTCOME_COLORS.get(o, C['gray']) for o in outcomes['journey_outcome']]\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    outcomes['sessions'], labels=outcomes['journey_outcome'],\n",
    "    colors=colors_list, autopct='%1.1f%%', startangle=90,\n",
    "    pctdistance=0.8, wedgeprops=dict(width=0.4))\n",
    "ax1.set_title('Session Journey Outcomes')\n",
    "\n",
    "# Bar chart\n",
    "y_pos = range(len(outcomes))\n",
    "bars = ax2.barh(\n",
    "    [o for o in reversed(outcomes['journey_outcome'])],\n",
    "    [s for s in reversed(outcomes['sessions'])],\n",
    "    color=[OUTCOME_COLORS.get(o, C['gray']) for o in reversed(outcomes['journey_outcome'])])\n",
    "for i, (idx, row) in enumerate(outcomes.iloc[::-1].iterrows()):\n",
    "    ax2.text(row['sessions'] + max(outcomes['sessions'])*0.01, i,\n",
    "             f\"{int(row['sessions']):,} ({row['pct']:.1f}%)\",\n",
    "             va='center', fontsize=10)\n",
    "ax2.set_xlabel('Sessions')\n",
    "ax2.set_title('Session Count by Outcome')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SESSION COMPLEXITY + BEHAVIORAL KPIs =====\n",
    "complexity_dist = query(\"\"\"\n",
    "    SELECT\n",
    "        session_complexity,\n",
    "        session_complexity_sort,\n",
    "        COUNT(*) as sessions,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct\n",
    "    FROM searches_journeys\n",
    "    GROUP BY session_complexity, session_complexity_sort\n",
    "    ORDER BY session_complexity_sort\n",
    "\"\"\")\n",
    "\n",
    "behavior = query(\"\"\"\n",
    "    SELECT\n",
    "        ROUND(100.0 * SUM(CASE WHEN had_reformulation THEN 1 ELSE 0 END)\n",
    "            / COUNT(*), 1) as reformulation_rate,\n",
    "        ROUND(100.0 * SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END)\n",
    "            / NULLIF(SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END), 0), 1) as null_recovery_rate,\n",
    "        SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END) as sessions_with_null,\n",
    "        SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END) as sessions_recovered,\n",
    "        ROUND(AVG(CASE WHEN sec_result_to_click IS NOT NULL THEN sec_result_to_click END), 1) as avg_sec_to_click,\n",
    "        ROUND(AVG(CASE WHEN sec_search_to_result IS NOT NULL THEN sec_search_to_result END), 2) as avg_sec_search_to_result\n",
    "    FROM searches_journeys\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Complexity distribution\n",
    "ccolors = [C['success_light'], C['neutral_light'], C['warn_light'], C['fail_light']][:len(complexity_dist)]\n",
    "ax1.bar(complexity_dist['session_complexity'], complexity_dist['sessions'], color=ccolors)\n",
    "ax1.set_ylabel('Sessions')\n",
    "ax1.set_title('Session Complexity Distribution')\n",
    "for i, (_, row) in enumerate(complexity_dist.iterrows()):\n",
    "    ax1.text(i, row['sessions'], f\"{row['pct']:.0f}%\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# KPI text card\n",
    "b = behavior.iloc[0]\n",
    "kpi_lines = [\n",
    "    f\"Reformulation Rate:      {fmt_pct(b['reformulation_rate'])}\",\n",
    "    f\"  (users who rephrased their query)\",\n",
    "    \"\",\n",
    "    f\"Null Recovery Rate:      {fmt_pct(b['null_recovery_rate'])}\",\n",
    "    f\"  ({fmt_num(b['sessions_recovered'])} of {fmt_num(b['sessions_with_null'])} null sessions)\",\n",
    "    \"\",\n",
    "    f\"Avg Search-to-Result:    {b['avg_sec_search_to_result']:.2f}s\",\n",
    "    f\"Avg Result-to-Click:     {b['avg_sec_to_click']:.1f}s\",\n",
    "]\n",
    "ax2.text(0.1, 0.5, '\\n'.join(kpi_lines), transform=ax2.transAxes,\n",
    "         fontsize=13, verticalalignment='center', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='#f5f5f5', alpha=0.8))\n",
    "ax2.set_title('Behavioral KPIs')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Journey analysis revealed that **[X]%** of sessions ended in Success (user found content), while **[Y]%** were Abandoned (results shown but nobody clicked). The reformulation rate of **[Z]%** shows how often users needed to rephrase queries -- a signal for search relevance improvements. The null recovery rate of **[W]%** tells us how resilient the search experience is: **[W]%** of users who initially got zero results were able to recover by rephrasing. Search latency averaged **[N]** seconds, well within acceptable limits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Regional & Temporal Patterns\n",
    "**Business Question:** When do employees search, and what does that tell us about our global workforce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TIME OF DAY DISTRIBUTION =====\n",
    "time_dist = query(\"\"\"\n",
    "    SELECT\n",
    "        SUM(searches_night) as \"APAC (03-09 CET)\",\n",
    "        SUM(searches_morning) as \"CET (09-16 CET)\",\n",
    "        SUM(searches_afternoon) as \"Americas (16-22 CET)\",\n",
    "        SUM(searches_evening) as \"Dead Time (22-03 CET)\"\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "labels = time_dist.columns.tolist()\n",
    "values = time_dist.iloc[0].values.astype(float)\n",
    "region_colors = [C['warn'], C['success'], C['neutral'], C['gray']]\n",
    "ax1.pie(values, labels=labels, colors=region_colors,\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Search Volume by Time Zone / Region')\n",
    "\n",
    "# Hour-of-day x day-of-week heatmap\n",
    "hourly_dow = query(\"\"\"\n",
    "    SELECT\n",
    "        event_weekday as weekday,\n",
    "        event_weekday_num as dow_num,\n",
    "        event_hour as hour,\n",
    "        COUNT(*) as searches\n",
    "    FROM searches\n",
    "    WHERE name = 'SEARCH_TRIGGERED'\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 2, 3\n",
    "\"\"\")\n",
    "\n",
    "if len(hourly_dow) > 0:\n",
    "    pivot = hourly_dow.pivot_table(index='weekday', columns='hour', values='searches',\n",
    "                                    aggfunc='sum', fill_value=0)\n",
    "    dow_order = hourly_dow.drop_duplicates('weekday').sort_values('dow_num')['weekday'].tolist()\n",
    "    pivot = pivot.reindex(dow_order)\n",
    "    sns.heatmap(pivot, cmap='YlOrRd', ax=ax2, fmt='.0f',\n",
    "                cbar_kws={'label': 'Searches'})\n",
    "    ax2.set_title('Search Heatmap: Day x Hour (CET)')\n",
    "    ax2.set_xlabel('Hour (CET)')\n",
    "    ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Peak hours\n",
    "peak_hours = query(\"\"\"\n",
    "    SELECT event_hour as hour, COUNT(*) as searches\n",
    "    FROM searches WHERE name = 'SEARCH_TRIGGERED'\n",
    "    GROUP BY 1 ORDER BY 2 DESC LIMIT 3\n",
    "\"\"\")\n",
    "print(\"Top 3 peak hours (CET):\")\n",
    "for _, row in peak_hours.iterrows():\n",
    "    print(f\"  {int(row['hour']):02d}:00 -- {int(row['searches']):,} searches\")\n",
    "\n",
    "# Regional percentages\n",
    "total_regional = values.sum()\n",
    "if total_regional > 0:\n",
    "    print(f\"\\nRegional breakdown:\")\n",
    "    for label, val in zip(labels, values):\n",
    "        print(f\"  {label}: {val:,.0f} ({100*val/total_regional:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Time-of-day analysis confirmed our intranet serves a global workforce. **[X]%** of searches occur during CET business hours (09-16), **[Y]%** during Americas hours (16-22 CET), and **[Z]%** during APAC hours (03-09 CET). The peak search hour is **[HH]:00 CET**. This data informed content publishing schedules -- publishing before the peak ensures content is fresh when most users search. The day-of-week heatmap revealed the exact windows of highest activity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Interview Cheat Sheet\n",
    "All key numbers compiled in one place for quick reference during STAR-based interview answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMPILE ALL KEY NUMBERS =====\n",
    "cheat = query(\"\"\"\n",
    "    WITH daily_totals AS (\n",
    "        SELECT\n",
    "            MIN(date) as date_from, MAX(date) as date_to,\n",
    "            COUNT(*) as days,\n",
    "            SUM(search_starts) as total_searches,\n",
    "            SUM(unique_users) as total_user_days,\n",
    "            SUM(unique_sessions) as total_sessions,\n",
    "            ROUND(100.0 * SUM(sessions_with_clicks) / NULLIF(SUM(sessions_with_results), 0), 1) as session_success_rate,\n",
    "            ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as null_result_rate,\n",
    "            ROUND(100.0 * SUM(sessions_abandoned) / NULLIF(SUM(sessions_with_results), 0), 1) as abandonment_rate,\n",
    "            ROUND(SUM(search_starts) * 1.0 / NULLIF(SUM(unique_sessions), 0), 2) as avg_searches_per_session,\n",
    "            SUM(null_results) as total_null_results,\n",
    "            SUM(new_users) as total_new_users,\n",
    "            SUM(returning_users) as total_returning_users\n",
    "        FROM searches_daily\n",
    "    ),\n",
    "    journey_totals AS (\n",
    "        SELECT\n",
    "            COUNT(*) as total_journeys,\n",
    "            ROUND(100.0 * SUM(CASE WHEN had_reformulation THEN 1 ELSE 0 END) / COUNT(*), 1) as reformulation_rate,\n",
    "            ROUND(100.0 * SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END)\n",
    "                / NULLIF(SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END), 0), 1) as null_recovery_rate,\n",
    "            ROUND(AVG(CASE WHEN sec_result_to_click IS NOT NULL THEN sec_result_to_click END), 1) as avg_sec_to_click,\n",
    "            ROUND(AVG(CASE WHEN sec_search_to_result IS NOT NULL THEN sec_search_to_result END), 2) as avg_latency,\n",
    "            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_success,\n",
    "            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Abandoned' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_abandoned,\n",
    "            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'No Results' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_no_results,\n",
    "            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Engaged' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_engaged\n",
    "        FROM searches_journeys\n",
    "    ),\n",
    "    term_totals AS (\n",
    "        SELECT\n",
    "            COUNT(DISTINCT search_term) as unique_terms,\n",
    "            (SELECT COUNT(*) FROM (\n",
    "                SELECT search_term FROM searches_terms GROUP BY search_term\n",
    "                HAVING SUM(null_result_count) = SUM(result_events) AND SUM(result_events) > 0\n",
    "            )) as pure_zero_result_terms\n",
    "        FROM searches_terms\n",
    "    )\n",
    "    SELECT * FROM daily_totals, journey_totals, term_totals\n",
    "\"\"\")\n",
    "\n",
    "c = cheat.iloc[0]\n",
    "\n",
    "cheat_sheet = pd.DataFrame([\n",
    "    ['SCOPE', ''],\n",
    "    ['  Period', f\"{c['date_from']} to {c['date_to']} ({int(c['days'])} days)\"],\n",
    "    ['  Total Searches', fmt_num(c['total_searches'])],\n",
    "    ['  User-Days', fmt_num(c['total_user_days'])],\n",
    "    ['  Total Sessions', fmt_num(c['total_sessions'])],\n",
    "    ['  Unique Search Terms', fmt_num(c['unique_terms'])],\n",
    "    ['', ''],\n",
    "    ['QUALITY METRICS', ''],\n",
    "    ['  Session Success Rate', fmt_pct(c['session_success_rate'])],\n",
    "    ['  Null Result Rate', fmt_pct(c['null_result_rate'])],\n",
    "    ['  Abandonment Rate', fmt_pct(c['abandonment_rate'])],\n",
    "    ['  Avg Searches/Session', f\"{c['avg_searches_per_session']:.2f}\"],\n",
    "    ['  Reformulation Rate', fmt_pct(c['reformulation_rate'])],\n",
    "    ['  Null Recovery Rate', fmt_pct(c['null_recovery_rate'])],\n",
    "    ['', ''],\n",
    "    ['JOURNEY OUTCOMES', ''],\n",
    "    ['  Success (clicked result)', fmt_pct(c['pct_success'])],\n",
    "    ['  Engaged (navigated, no click)', fmt_pct(c['pct_engaged'])],\n",
    "    ['  Abandoned (results, no action)', fmt_pct(c['pct_abandoned'])],\n",
    "    ['  No Results', fmt_pct(c['pct_no_results'])],\n",
    "    ['', ''],\n",
    "    ['PERFORMANCE', ''],\n",
    "    ['  Avg Search Latency', f\"{c['avg_latency']:.2f}s\"],\n",
    "    ['  Avg Time to Click', f\"{c['avg_sec_to_click']:.1f}s\"],\n",
    "    ['', ''],\n",
    "    ['CONTENT GAPS', ''],\n",
    "    ['  Total Failed Searches', fmt_num(c['total_null_results'])],\n",
    "    ['  Terms Always Returning Zero', fmt_num(c['pure_zero_result_terms'])],\n",
    "    ['', ''],\n",
    "    ['USER ADOPTION', ''],\n",
    "    ['  New Users', f\"{fmt_num(c['total_new_users'])} ({100*c['total_new_users']/(c['total_new_users']+c['total_returning_users']):.0f}%)\"],\n",
    "    ['  Returning Users', f\"{fmt_num(c['total_returning_users'])} ({100*c['total_returning_users']/(c['total_new_users']+c['total_returning_users']):.0f}%)\"],\n",
    "], columns=['Metric', 'Value'])\n",
    "\n",
    "print(\"=\" * 56)\n",
    "print(\"  INTERVIEW REFERENCE: ALL KEY NUMBERS\")\n",
    "print(\"=\" * 56)\n",
    "for _, row in cheat_sheet.iterrows():\n",
    "    if row['Value'] == '':\n",
    "        print(f\"\\n  {row['Metric']}\")\n",
    "    else:\n",
    "        print(f\"  {row['Metric']:<40s} {row['Value']}\")\n",
    "print(\"\\n\" + \"=\" * 56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAR Talking Points\n",
    "\n",
    "Fill in the numbers from above and rehearse these four stories:\n",
    "\n",
    "---\n",
    "\n",
    "#### Story 1: Building the Analytics Pipeline (Technical Leadership)\n",
    "- **S:** Our intranet search had no analytics -- zero visibility into what employees were searching for or whether they found content.\n",
    "- **T:** I was tasked with designing and building an end-to-end search analytics solution from raw telemetry to executive dashboards.\n",
    "- **A:** I built a Python ETL pipeline (1,000+ lines) that ingests Azure App Insights telemetry, enriches it with session analysis and journey classification, and outputs analytics-ready data. I designed 30+ DAX measures for Power BI and created 5 dashboard pages. I documented the complete data model and visualization guide.\n",
    "- **R:** We went from zero visibility to tracking **[total_searches]** searches across **[users]** users, with automated session classification, content gap detection, and quality scorecards. The solution now runs weekly and serves senior management.\n",
    "\n",
    "---\n",
    "\n",
    "#### Story 2: Identifying Content Gaps (Business Impact)\n",
    "- **S:** Internal Communications had no data-driven way to decide what intranet content to create or improve.\n",
    "- **T:** Use search analytics to identify the most impactful content gaps.\n",
    "- **A:** I analyzed zero-result search terms and identified **[pure_zero_result_terms]** unique terms that consistently return no results, ranked by volume.\n",
    "- **R:** This gave Internal Comms a prioritized content roadmap. Addressing just the top 10 gaps would cover **[X]%** of all gap-related searches. Each term on the list represents a direct, measurable improvement opportunity.\n",
    "\n",
    "---\n",
    "\n",
    "#### Story 3: Measuring Search Quality (Data-Driven Decisions)\n",
    "- **S:** Leadership needed to know if the intranet was actually working -- no baseline metrics existed.\n",
    "- **T:** Define and implement quality KPIs that an executive audience would understand and act on.\n",
    "- **A:** I designed a scorecard with Session Success Rate (**[X]%**), Null Result Rate (**[Y]%**), and Abandonment Rate (**[Z]%**). I set benchmark targets based on industry standards and trended these weekly.\n",
    "- **R:** For the first time, leadership could see a single number (session success rate) and understand whether search was improving. The reformulation rate of **[W]%** became a key input for the search team's optimization roadmap.\n",
    "\n",
    "---\n",
    "\n",
    "#### Story 4: Global Workforce Insights (Strategic Thinking)\n",
    "- **S:** We assumed our intranet served primarily a European audience but had no data.\n",
    "- **T:** Analyze temporal patterns to understand global usage.\n",
    "- **A:** I implemented CET-based time zone analysis, classifying searches into APAC, CET, and Americas windows.\n",
    "- **R:** Data revealed the actual regional split, informing content publishing schedules and multilingual content priorities. Publishing before peak hour ensures maximum reach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CLEANUP =====\n",
    "con.close()\n",
    "print(\"Database connection closed. Notebook complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
