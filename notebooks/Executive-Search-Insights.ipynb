{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Search Insights\n## Intranet Search Analytics -- Interview Reference\n\nThis notebook extracts key metrics from the intranet search analytics database and presents them as **executive-level findings**, organized for use with the **STAR interview framework**.\n\n**Prerequisites:** The DuckDB database must exist at `../data/searchanalytics.db`. Run `python process_search_analytics.py` first.\n\n**Audience:** Internal Communications Executive\n\n**Sections:**\n1. Executive Summary Dashboard\n2. Search Volume & Adoption Trends\n3. Search Quality Scorecard\n4. Content Gap Analysis (The Big Win)\n5. Search Term Performance\n6. User Journey & Behavior Insights\n7. Regional & Temporal Patterns\n8. Search Ranking & Performance\n9. Audience Intelligence\n10. Content & Navigation\n11. Content Discovery: Term \u2192 Clicked Content\n12. Interview Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SETUP & DATABASE CONNECTION =====\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Color scheme\n",
    "C = {\n",
    "    'success': '#2E7D32', 'success_light': '#81C784',\n",
    "    'fail': '#C62828', 'fail_light': '#EF9A9A',\n",
    "    'warn': '#F57C00', 'warn_light': '#FFB74D',\n",
    "    'neutral': '#1565C0', 'neutral_light': '#64B5F6',\n",
    "    'gray': '#757575', 'gray_light': '#E0E0E0',\n",
    "}\n",
    "OUTCOME_COLORS = {\n",
    "    'Success': C['success'], 'Engaged': C['neutral'],\n",
    "    'Abandoned': C['warn'], 'No Results': C['fail'], 'Unknown': C['gray'],\n",
    "}\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.1f}')\n",
    "\n",
    "# Connect\n",
    "db_path = Path('../data/searchanalytics.db')\n",
    "if not db_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Database not found at {db_path}\\n\"\n",
    "        \"Run: python process_search_analytics.py\")\n",
    "\n",
    "con = duckdb.connect(str(db_path), read_only=True)\n",
    "\n",
    "def query(sql):\n",
    "    \"\"\"Execute SQL and return DataFrame\"\"\"\n",
    "    return con.execute(sql).df()\n",
    "\n",
    "def fmt_pct(val, decimals=1):\n",
    "    if pd.isna(val): return 'N/A'\n",
    "    return f\"{val:.{decimals}f}%\"\n",
    "\n",
    "def fmt_num(val):\n",
    "    if pd.isna(val): return 'N/A'\n",
    "    return f\"{int(val):,}\"\n",
    "\n",
    "# Verify tables\n",
    "tables = query(\"SHOW TABLES\")['name'].tolist()\n",
    "required = ['searches', 'searches_daily', 'searches_journeys', 'searches_terms']\n",
    "missing = [t for t in required if t not in tables]\n",
    "if missing:\n",
    "    print(f\"WARNING: Missing tables: {missing}\")\n",
    "    print(\"Run: python process_search_analytics.py --full-refresh\")\n",
    "else:\n",
    "    for t in required:\n",
    "        n = query(f\"SELECT COUNT(*) as n FROM {t}\")['n'][0]\n",
    "        print(f\"  {t}: {n:,} rows\")\n",
    "\n",
    "date_range = query(\"\"\"\n",
    "    SELECT MIN(session_date) as first_date, MAX(session_date) as last_date,\n",
    "           COUNT(DISTINCT session_date) as days\n",
    "    FROM searches\n",
    "\"\"\")\n",
    "print(f\"\\nDate range: {date_range['first_date'][0]} to {date_range['last_date'][0]} ({date_range['days'][0]} days)\")\n",
    "print(\"\\nReady.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Executive Summary Dashboard\n",
    "**Business Question:** What is the overall health of our intranet search, at a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXECUTIVE SUMMARY KPIs =====\n",
    "summary = query(\"\"\"\n",
    "    SELECT\n",
    "        MIN(date) as date_from,\n",
    "        MAX(date) as date_to,\n",
    "        COUNT(*) as days_covered,\n",
    "        SUM(search_starts) as total_searches,\n",
    "        SUM(unique_users) as total_user_days,\n",
    "        SUM(unique_sessions) as total_sessions,\n",
    "        SUM(unique_search_terms) as total_unique_terms,\n",
    "        -- Session Success Rate % = sessions_with_clicks / sessions_with_results\n",
    "        ROUND(100.0 * SUM(sessions_with_clicks) / NULLIF(SUM(sessions_with_results), 0), 1) as session_success_rate,\n",
    "        -- Null Result Rate % = null_results / result_events\n",
    "        ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as null_result_rate,\n",
    "        -- Abandonment Rate % = sessions_abandoned / sessions_with_results\n",
    "        ROUND(100.0 * SUM(sessions_abandoned) / NULLIF(SUM(sessions_with_results), 0), 1) as abandonment_rate,\n",
    "        -- Avg Searches per Session\n",
    "        ROUND(SUM(search_starts) * 1.0 / NULLIF(SUM(unique_sessions), 0), 2) as avg_searches_per_session,\n",
    "        -- Reformulation rate from journeys\n",
    "        ROUND(100.0 * (SELECT COUNT(*) FROM searches_journeys WHERE had_reformulation = true)\n",
    "            / NULLIF((SELECT COUNT(*) FROM searches_journeys), 0), 1) as reformulation_rate\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "s = summary.iloc[0]\n",
    "print(\"=\" * 64)\n",
    "print(\"  INTRANET SEARCH ANALYTICS -- EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 64)\n",
    "print(f\"  Period:     {s['date_from']} to {s['date_to']} ({int(s['days_covered'])} days)\")\n",
    "print(f\"  Searches:   {fmt_num(s['total_searches'])}\")\n",
    "print(f\"  Users:      {fmt_num(s['total_user_days'])} user-days\")\n",
    "print(f\"  Sessions:   {fmt_num(s['total_sessions'])}\")\n",
    "print(\"-\" * 64)\n",
    "print(f\"  Session Success Rate:    {fmt_pct(s['session_success_rate'])}\")\n",
    "print(f\"  Null Result Rate:        {fmt_pct(s['null_result_rate'])}\")\n",
    "print(f\"  Abandonment Rate:        {fmt_pct(s['abandonment_rate'])}\")\n",
    "print(f\"  Avg Searches/Session:    {s['avg_searches_per_session']:.2f}\")\n",
    "print(f\"  Reformulation Rate:      {fmt_pct(s['reformulation_rate'])}\")\n",
    "print(\"=\" * 64)\n",
    "\n",
    "# Store for cheat sheet\n",
    "EXEC = s.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I built a search analytics solution tracking **[total_searches]** searches across **[users]** users over **[days]** days. The key finding: our session success rate was **[X]%**, meaning roughly **[Y in Z]** search sessions resulted in users clicking on actual content. The null result rate of **[X]%** represented our biggest content gap opportunity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Search Volume & Adoption Trends\n",
    "**Business Question:** Is intranet search usage growing? How are users adopting the platform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SEARCH VOLUME OVER TIME =====\n",
    "daily = query(\"\"\"\n",
    "    SELECT date, search_starts, unique_users, unique_sessions,\n",
    "           day_of_week, day_of_week_num, new_users, returning_users\n",
    "    FROM searches_daily\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 5))\n",
    "ax1.bar(daily['date'], daily['search_starts'], color=C['neutral_light'],\n",
    "        alpha=0.7, label='Daily Searches', width=0.8)\n",
    "ax1.set_ylabel('Search Count', color=C['neutral'])\n",
    "ax1.tick_params(axis='y', labelcolor=C['neutral'])\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "ax1.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(daily['date'], daily['unique_users'], color=C['success'],\n",
    "         linewidth=2, marker='o', markersize=3, label='Unique Users')\n",
    "ax2.set_ylabel('Unique Users', color=C['success'])\n",
    "ax2.tick_params(axis='y', labelcolor=C['success'])\n",
    "\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.12, 0.95))\n",
    "plt.title('Daily Search Volume & Unique Users')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Growth rate\n",
    "if len(daily) >= 14:\n",
    "    first_week = daily.head(7)['search_starts'].sum()\n",
    "    last_week = daily.tail(7)['search_starts'].sum()\n",
    "    growth = ((last_week - first_week) / first_week * 100) if first_week > 0 else 0\n",
    "    print(f\"Growth (first week vs last week): {growth:+.1f}%\")\n",
    "    print(f\"  First 7 days: {first_week:,} searches\")\n",
    "    print(f\"  Last 7 days:  {last_week:,} searches\")\n",
    "\n",
    "avg_daily = daily['search_starts'].mean()\n",
    "avg_users = daily['unique_users'].mean()\n",
    "print(f\"\\nAverage daily searches: {avg_daily:,.0f}\")\n",
    "print(f\"Average daily unique users: {avg_users:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== NEW vs RETURNING USERS =====\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.fill_between(daily['date'], 0, daily['returning_users'],\n",
    "                color=C['neutral_light'], alpha=0.7, label='Returning Users')\n",
    "ax.fill_between(daily['date'], daily['returning_users'],\n",
    "                daily['returning_users'] + daily['new_users'],\n",
    "                color=C['success_light'], alpha=0.7, label='New Users')\n",
    "ax.set_ylabel('Users')\n",
    "ax.set_title('New vs Returning Users Over Time')\n",
    "ax.legend(loc='upper left')\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_new = daily['new_users'].sum()\n",
    "total_returning = daily['returning_users'].sum()\n",
    "total_all = total_new + total_returning\n",
    "if total_all > 0:\n",
    "    print(f\"New users:       {total_new:,} ({100*total_new/total_all:.1f}%)\")\n",
    "    print(f\"Returning users: {total_returning:,} ({100*total_returning/total_all:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DAY OF WEEK PATTERNS =====\n",
    "dow = daily.groupby(['day_of_week', 'day_of_week_num']).agg(\n",
    "    avg_searches=('search_starts', 'mean'),\n",
    "    total_searches=('search_starts', 'sum'),\n",
    "    avg_users=('unique_users', 'mean'),\n",
    ").reset_index().sort_values('day_of_week_num')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(dow['day_of_week'], dow['avg_searches'], color=C['neutral'])\n",
    "for i, (_, row) in enumerate(dow.iterrows()):\n",
    "    if row['day_of_week_num'] >= 6:\n",
    "        bars[i].set_color(C['gray_light'])\n",
    "ax.set_ylabel('Average Daily Searches')\n",
    "ax.set_title('Search Volume by Day of Week')\n",
    "for bar in bars:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{bar.get_height():.0f}', ha='center', va='bottom', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "busiest = dow.loc[dow['avg_searches'].idxmax()]\n",
    "quietest = dow.loc[dow['avg_searches'].idxmin()]\n",
    "print(f\"Busiest day:  {busiest['day_of_week']} ({busiest['avg_searches']:.0f} avg searches)\")\n",
    "print(f\"Quietest day: {quietest['day_of_week']} ({quietest['avg_searches']:.0f} avg searches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Search adoption showed **[growth trend]** over the measurement period. We tracked **[X]** unique user-days, with **[Y]%** being returning users -- demonstrating strong repeat engagement. **[Busiest day]** was consistently the peak day with **[N]** average daily searches, while weekends showed minimal activity, confirming this is a workplace productivity tool.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Search Quality Scorecard\n",
    "**Business Question:** Are users finding what they need? Is search quality improving over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUALITY METRICS TRENDED =====\n",
    "quality = query(\"\"\"\n",
    "    SELECT date, sessions_with_clicks, sessions_with_results, sessions_abandoned,\n",
    "           null_results, result_events, search_starts, success_clicks, unique_sessions\n",
    "    FROM searches_daily\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "quality['date'] = pd.to_datetime(quality['date'])\n",
    "\n",
    "# Daily rates\n",
    "quality['session_success_rate'] = 100.0 * quality['sessions_with_clicks'] / quality['sessions_with_results'].replace(0, np.nan)\n",
    "quality['null_result_rate'] = 100.0 * quality['null_results'] / quality['result_events'].replace(0, np.nan)\n",
    "quality['abandonment_rate'] = 100.0 * quality['sessions_abandoned'] / quality['sessions_with_results'].replace(0, np.nan)\n",
    "\n",
    "# 7-day rolling averages\n",
    "for col in ['session_success_rate', 'null_result_rate', 'abandonment_rate']:\n",
    "    quality[f'{col}_7d'] = quality[col].rolling(7, min_periods=1).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(quality['date'], quality['session_success_rate_7d'],\n",
    "        color=C['success'], linewidth=2.5, label='Session Success Rate %')\n",
    "ax.plot(quality['date'], quality['null_result_rate_7d'],\n",
    "        color=C['fail'], linewidth=2.5, label='Null Result Rate %')\n",
    "ax.plot(quality['date'], quality['abandonment_rate_7d'],\n",
    "        color=C['warn'], linewidth=2.5, label='Abandonment Rate %')\n",
    "\n",
    "ax.set_ylabel('Rate (%)')\n",
    "ax.set_title('Search Quality Metrics -- 7-Day Rolling Average')\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall assessment\n",
    "overall_success = 100.0 * quality['sessions_with_clicks'].sum() / quality['sessions_with_results'].sum()\n",
    "overall_null = 100.0 * quality['null_results'].sum() / quality['result_events'].sum()\n",
    "overall_abandon = 100.0 * quality['sessions_abandoned'].sum() / quality['sessions_with_results'].sum()\n",
    "\n",
    "assessment = pd.DataFrame({\n",
    "    'Metric': ['Session Success Rate', 'Null Result Rate', 'Abandonment Rate'],\n",
    "    'Value': [f'{overall_success:.1f}%', f'{overall_null:.1f}%', f'{overall_abandon:.1f}%'],\n",
    "    'Target': ['>40%', '<5%', '<50%'],\n",
    "    'Assessment': [\n",
    "        'Good' if overall_success > 40 else ('Fair' if overall_success > 25 else 'Needs Improvement'),\n",
    "        'Good' if overall_null < 5 else ('Fair' if overall_null < 15 else 'Needs Improvement'),\n",
    "        'Good' if overall_abandon < 50 else ('Fair' if overall_abandon < 70 else 'Needs Improvement'),\n",
    "    ]\n",
    "})\n",
    "print(\"\\nOverall Quality Assessment:\")\n",
    "display(assessment.style.hide(axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I established a search quality scorecard with three headline metrics. Session Success Rate was **[X]%** (target >40%), Null Result Rate was **[Y]%** (target <5%), and Abandonment Rate was **[Z]%** (target <50%). The 7-day rolling average showed **[improving/stable/declining]** quality over time, providing a clear signal for whether content and search improvements were working.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Content Gap Analysis (The Big Win)\n",
    "**Business Question:** What are employees searching for that we have no content for? Where is the biggest opportunity for Internal Communications?\n",
    "\n",
    "This is the most actionable section. Each zero-result term represents a **content gap** -- employees actively looking for information that does not exist on the intranet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOP ZERO-RESULT SEARCH TERMS =====\n",
    "zero_results = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term,\n",
    "        SUM(search_count) as total_searches,\n",
    "        SUM(null_result_count) as null_count,\n",
    "        SUM(result_events) as result_events,\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as null_rate,\n",
    "        SUM(unique_users) as unique_users_searching\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(null_result_count) > 0\n",
    "       AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) >= 50\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display_df = zero_results[['search_term', 'total_searches', 'null_count', 'null_rate', 'unique_users_searching']].rename(columns={\n",
    "    'search_term': 'Search Term',\n",
    "    'total_searches': 'Total Searches',\n",
    "    'null_count': 'Zero-Result Count',\n",
    "    'null_rate': 'Null Rate %',\n",
    "    'unique_users_searching': 'Unique Users',\n",
    "})\n",
    "\n",
    "print(\"TOP 20 CONTENT GAPS: Search Terms with >= 50% Null Rate\")\n",
    "print(\"Each row = employees searching for content that DOES NOT EXIST.\\n\")\n",
    "display(display_df.style.hide(axis='index')\n",
    "    .format({'Total Searches': '{:,}', 'Zero-Result Count': '{:,}',\n",
    "             'Null Rate %': '{:.1f}%', 'Unique Users': '{:,}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FAILED SEARCH VOLUME SUMMARY =====\n",
    "gap_summary = query(\"\"\"\n",
    "    SELECT\n",
    "        SUM(null_results) as total_failed,\n",
    "        SUM(result_events) as total_results,\n",
    "        ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as overall_null_rate\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "pure_zero = query(\"\"\"\n",
    "    SELECT COUNT(*) as cnt FROM (\n",
    "        SELECT search_term\n",
    "        FROM searches_terms\n",
    "        GROUP BY search_term\n",
    "        HAVING SUM(null_result_count) = SUM(result_events) AND SUM(result_events) > 0\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Volume covered by top 10 gaps\n",
    "top10_gap_volume = zero_results.head(10)['total_searches'].sum() if len(zero_results) >= 10 else zero_results['total_searches'].sum()\n",
    "all_gap_volume = query(\"\"\"\n",
    "    SELECT SUM(search_count) as vol FROM (\n",
    "        SELECT search_term, SUM(search_count) as search_count, SUM(null_result_count) as nulls, SUM(result_events) as results\n",
    "        FROM searches_terms GROUP BY search_term\n",
    "        HAVING SUM(null_result_count) > 0 AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) >= 50\n",
    "    )\n",
    "\"\"\")['vol'][0]\n",
    "top10_pct = 100.0 * top10_gap_volume / all_gap_volume if all_gap_volume > 0 else 0\n",
    "\n",
    "gs = gap_summary.iloc[0]\n",
    "print(f\"Total failed searches (zero results):      {fmt_num(gs['total_failed'])}\")\n",
    "print(f\"Overall null result rate:                   {fmt_pct(gs['overall_null_rate'])}\")\n",
    "print(f\"Unique terms that ALWAYS return zero:       {fmt_num(pure_zero['cnt'][0])}\")\n",
    "print(f\"Top 10 gap terms cover:                     {top10_pct:.0f}% of all gap-related searches\")\n",
    "\n",
    "# Chart: horizontal bar of top content gaps\n",
    "if len(zero_results) > 0:\n",
    "    top_n = zero_results.head(15).sort_values('total_searches')\n",
    "    fig, ax = plt.subplots(figsize=(12, max(5, len(top_n) * 0.45)))\n",
    "    ax.barh(top_n['search_term'], top_n['total_searches'], color=C['fail'])\n",
    "    ax.set_xlabel('Total Searches (returning zero results)')\n",
    "    ax.set_title('Top Content Gaps: Most-Searched Terms with Zero Results')\n",
    "    for i, (_, row) in enumerate(top_n.iterrows()):\n",
    "        ax.text(row['total_searches'] + 0.3, i, f\"{int(row['total_searches']):,}\",\n",
    "                va='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready) -- THE BIG WIN\n",
    "\n",
    "> **SITUATION:** Employees were searching the intranet but Internal Communications had no data on what content was missing.\n",
    ">\n",
    "> **TASK:** I built a search analytics pipeline to identify content gaps and measure search quality.\n",
    ">\n",
    "> **ACTION:** I analyzed all search terms that consistently returned zero results. I identified **[N]** unique terms that always fail and ranked them by search volume so the content team could prioritize.\n",
    ">\n",
    "> **RESULT:** The top content gap, \"[top term]\", was searched **[X]** times with zero results. Addressing just the top 10 terms would cover **[Y]%** of all gap-related search volume. This gave Internal Comms their first ever **data-driven content roadmap**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Search Term Performance\n",
    "**Business Question:** Which search terms are working well? Which need attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TOP 20 TERMS BY VOLUME WITH SUCCESS RATES =====\n",
    "top_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        SUM(success_click_count) as \"Success Clicks\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        SUM(null_result_count) as \"Null Results\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\",\n",
    "        SUM(unique_users) as \"Unique Users\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 3\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 20 Search Terms by Volume\")\n",
    "display(top_terms.style.hide(axis='index')\n",
    "    .format({'Searches': '{:,}', 'Success Clicks': '{:,}',\n",
    "             'Success CTR %': '{:.1f}%', 'Null Results': '{:,}',\n",
    "             'Null Rate %': '{:.1f}%', 'Unique Users': '{:,}'})\n",
    "    .bar(subset=['Success CTR %'], color=C['success_light'], vmin=0, vmax=100)\n",
    "    .bar(subset=['Null Rate %'], color=C['fail_light'], vmin=0, vmax=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PROBLEM TERMS vs SUCCESS STORIES =====\n",
    "\n",
    "# Problem: results exist but nobody clicks (relevance issue)\n",
    "problem_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 5\n",
    "       AND ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) < 20\n",
    "       AND ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) < 50\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(\"RELEVANCE PROBLEMS: Results shown but users don't click (CTR < 20%, Null Rate < 50%)\")\n",
    "print(\"Action: Improve content titles, metadata, or search ranking\\n\")\n",
    "if len(problem_terms) > 0:\n",
    "    display(problem_terms.style.hide(axis='index')\n",
    "        .format({'Searches': '{:,}', 'Success CTR %': '{:.1f}%', 'Null Rate %': '{:.1f}%'}))\n",
    "else:\n",
    "    print(\"No terms matching these criteria (good news!)\")\n",
    "\n",
    "# Success stories: high volume + high CTR\n",
    "success_terms = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term as \"Search Term\",\n",
    "        SUM(search_count) as \"Searches\",\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as \"Success CTR %\",\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as \"Null Rate %\"\n",
    "    FROM searches_terms\n",
    "    GROUP BY search_term\n",
    "    HAVING SUM(search_count) >= 5\n",
    "       AND ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) >= 30\n",
    "    ORDER BY SUM(search_count) DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSUCCESS STORIES: High click-through terms (CTR >= 30%)\")\n",
    "print(\"These demonstrate what 'good' looks like -- benchmark for other content\\n\")\n",
    "if len(success_terms) > 0:\n",
    "    display(success_terms.style.hide(axis='index')\n",
    "        .format({'Searches': '{:,}', 'Success CTR %': '{:.1f}%', 'Null Rate %': '{:.1f}%'}))\n",
    "else:\n",
    "    print(\"No terms matching these criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== QUERY COMPLEXITY: Word count vs success =====\n",
    "complexity = query(\"\"\"\n",
    "    SELECT\n",
    "        CASE\n",
    "            WHEN word_count = 1 THEN '1 word'\n",
    "            WHEN word_count = 2 THEN '2 words'\n",
    "            WHEN word_count = 3 THEN '3 words'\n",
    "            WHEN word_count >= 4 THEN '4+ words'\n",
    "        END as query_length,\n",
    "        CASE\n",
    "            WHEN word_count = 1 THEN 1\n",
    "            WHEN word_count = 2 THEN 2\n",
    "            WHEN word_count = 3 THEN 3\n",
    "            WHEN word_count >= 4 THEN 4\n",
    "        END as sort_order,\n",
    "        SUM(search_count) as searches,\n",
    "        ROUND(100.0 * SUM(success_click_count) / NULLIF(SUM(search_count), 0), 1) as success_ctr,\n",
    "        ROUND(100.0 * SUM(null_result_count) / NULLIF(SUM(result_events), 0), 1) as null_rate,\n",
    "        ROUND(100.0 * SUM(search_count) / NULLIF((SELECT SUM(search_count) FROM searches_terms), 0), 1) as pct_of_total\n",
    "    FROM searches_terms\n",
    "    GROUP BY 1, 2\n",
    "    ORDER BY 2\n",
    "\"\"\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Volume distribution\n",
    "axes[0].bar(complexity['query_length'], complexity['pct_of_total'], color=C['neutral'])\n",
    "axes[0].set_ylabel('% of Total Searches')\n",
    "axes[0].set_title('Query Length Distribution')\n",
    "for i, v in enumerate(complexity['pct_of_total']):\n",
    "    axes[0].text(i, v + 0.3, f'{v:.0f}%', ha='center', fontsize=10)\n",
    "\n",
    "# CTR by length\n",
    "axes[1].bar(complexity['query_length'], complexity['success_ctr'], color=C['success'])\n",
    "axes[1].set_ylabel('Success CTR %')\n",
    "axes[1].set_title('Success Rate by Query Length')\n",
    "for i, v in enumerate(complexity['success_ctr']):\n",
    "    axes[1].text(i, v + 0.3, f'{v:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "# Null rate by length\n",
    "axes[2].bar(complexity['query_length'], complexity['null_rate'], color=C['fail'])\n",
    "axes[2].set_ylabel('Null Result Rate %')\n",
    "axes[2].set_title('Null Rate by Query Length')\n",
    "for i, v in enumerate(complexity['null_rate']):\n",
    "    axes[2].text(i, v + 0.3, f'{v:.1f}%', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_length = complexity.loc[complexity['success_ctr'].idxmax()]\n",
    "print(f\"Best performing query length: {best_length['query_length']} (CTR: {best_length['success_ctr']:.1f}%)\")\n",
    "most_common = complexity.loc[complexity['searches'].idxmax()]\n",
    "print(f\"Most common query length: {most_common['query_length']} ({most_common['pct_of_total']:.0f}% of all searches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"I categorized search terms into three groups: **Success Stories** (CTR > 30%), **Relevance Problems** (results exist but CTR < 20%), and **Content Gaps** (zero results). This framework gave Internal Comms a clear action plan for each category: create content for gaps, improve metadata for relevance problems, and replicate what works from success stories. **[Best length]**-word queries performed best at **[X]% CTR**, suggesting users get better results with more specific queries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. User Journey & Behavior Insights\n",
    "**Business Question:** How do search sessions play out? Where do users struggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== JOURNEY OUTCOME DISTRIBUTION =====\n",
    "outcomes = query(\"\"\"\n",
    "    SELECT\n",
    "        journey_outcome,\n",
    "        journey_outcome_sort,\n",
    "        COUNT(*) as sessions,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct\n",
    "    FROM searches_journeys\n",
    "    GROUP BY journey_outcome, journey_outcome_sort\n",
    "    ORDER BY journey_outcome_sort\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Donut chart\n",
    "colors_list = [OUTCOME_COLORS.get(o, C['gray']) for o in outcomes['journey_outcome']]\n",
    "wedges, texts, autotexts = ax1.pie(\n",
    "    outcomes['sessions'], labels=outcomes['journey_outcome'],\n",
    "    colors=colors_list, autopct='%1.1f%%', startangle=90,\n",
    "    pctdistance=0.8, wedgeprops=dict(width=0.4))\n",
    "ax1.set_title('Session Journey Outcomes')\n",
    "\n",
    "# Bar chart\n",
    "y_pos = range(len(outcomes))\n",
    "bars = ax2.barh(\n",
    "    [o for o in reversed(outcomes['journey_outcome'])],\n",
    "    [s for s in reversed(outcomes['sessions'])],\n",
    "    color=[OUTCOME_COLORS.get(o, C['gray']) for o in reversed(outcomes['journey_outcome'])])\n",
    "for i, (idx, row) in enumerate(outcomes.iloc[::-1].iterrows()):\n",
    "    ax2.text(row['sessions'] + max(outcomes['sessions'])*0.01, i,\n",
    "             f\"{int(row['sessions']):,} ({row['pct']:.1f}%)\",\n",
    "             va='center', fontsize=10)\n",
    "ax2.set_xlabel('Sessions')\n",
    "ax2.set_title('Session Count by Outcome')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SESSION COMPLEXITY + BEHAVIORAL KPIs =====\n",
    "complexity_dist = query(\"\"\"\n",
    "    SELECT\n",
    "        session_complexity,\n",
    "        session_complexity_sort,\n",
    "        COUNT(*) as sessions,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct\n",
    "    FROM searches_journeys\n",
    "    GROUP BY session_complexity, session_complexity_sort\n",
    "    ORDER BY session_complexity_sort\n",
    "\"\"\")\n",
    "\n",
    "behavior = query(\"\"\"\n",
    "    SELECT\n",
    "        ROUND(100.0 * SUM(CASE WHEN had_reformulation THEN 1 ELSE 0 END)\n",
    "            / COUNT(*), 1) as reformulation_rate,\n",
    "        ROUND(100.0 * SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END)\n",
    "            / NULLIF(SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END), 0), 1) as null_recovery_rate,\n",
    "        SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END) as sessions_with_null,\n",
    "        SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END) as sessions_recovered,\n",
    "        ROUND(AVG(CASE WHEN sec_result_to_click IS NOT NULL THEN sec_result_to_click END), 1) as avg_sec_to_click,\n",
    "        ROUND(AVG(CASE WHEN sec_search_to_result IS NOT NULL THEN sec_search_to_result END), 2) as avg_sec_search_to_result\n",
    "    FROM searches_journeys\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Complexity distribution\n",
    "ccolors = [C['success_light'], C['neutral_light'], C['warn_light'], C['fail_light']][:len(complexity_dist)]\n",
    "ax1.bar(complexity_dist['session_complexity'], complexity_dist['sessions'], color=ccolors)\n",
    "ax1.set_ylabel('Sessions')\n",
    "ax1.set_title('Session Complexity Distribution')\n",
    "for i, (_, row) in enumerate(complexity_dist.iterrows()):\n",
    "    ax1.text(i, row['sessions'], f\"{row['pct']:.0f}%\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# KPI text card\n",
    "b = behavior.iloc[0]\n",
    "kpi_lines = [\n",
    "    f\"Reformulation Rate:      {fmt_pct(b['reformulation_rate'])}\",\n",
    "    f\"  (users who rephrased their query)\",\n",
    "    \"\",\n",
    "    f\"Null Recovery Rate:      {fmt_pct(b['null_recovery_rate'])}\",\n",
    "    f\"  ({fmt_num(b['sessions_recovered'])} of {fmt_num(b['sessions_with_null'])} null sessions)\",\n",
    "    \"\",\n",
    "    f\"Avg Search-to-Result:    {b['avg_sec_search_to_result']:.2f}s\",\n",
    "    f\"Avg Result-to-Click:     {b['avg_sec_to_click']:.1f}s\",\n",
    "]\n",
    "ax2.text(0.1, 0.5, '\\n'.join(kpi_lines), transform=ax2.transAxes,\n",
    "         fontsize=13, verticalalignment='center', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='#f5f5f5', alpha=0.8))\n",
    "ax2.set_title('Behavioral KPIs')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Journey analysis revealed that **[X]%** of sessions ended in Success (user found content), while **[Y]%** were Abandoned (results shown but nobody clicked). The reformulation rate of **[Z]%** shows how often users needed to rephrase queries -- a signal for search relevance improvements. The null recovery rate of **[W]%** tells us how resilient the search experience is: **[W]%** of users who initially got zero results were able to recover by rephrasing. Search latency averaged **[N]** seconds, well within acceptable limits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Regional & Temporal Patterns\n",
    "**Business Question:** When do employees search, and what does that tell us about our global workforce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TIME OF DAY DISTRIBUTION =====\n",
    "time_dist = query(\"\"\"\n",
    "    SELECT\n",
    "        SUM(searches_night) as \"APAC (03-09 CET)\",\n",
    "        SUM(searches_morning) as \"CET (09-16 CET)\",\n",
    "        SUM(searches_afternoon) as \"Americas (16-22 CET)\",\n",
    "        SUM(searches_evening) as \"Dead Time (22-03 CET)\"\n",
    "    FROM searches_daily\n",
    "\"\"\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "labels = time_dist.columns.tolist()\n",
    "values = time_dist.iloc[0].values.astype(float)\n",
    "region_colors = [C['warn'], C['success'], C['neutral'], C['gray']]\n",
    "ax1.pie(values, labels=labels, colors=region_colors,\n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Search Volume by Time Zone / Region')\n",
    "\n",
    "# Hour-of-day x day-of-week heatmap\n",
    "hourly_dow = query(\"\"\"\n",
    "    SELECT\n",
    "        event_weekday as weekday,\n",
    "        event_weekday_num as dow_num,\n",
    "        event_hour as hour,\n",
    "        COUNT(*) as searches\n",
    "    FROM searches\n",
    "    WHERE name = 'SEARCH_TRIGGERED'\n",
    "    GROUP BY 1, 2, 3\n",
    "    ORDER BY 2, 3\n",
    "\"\"\")\n",
    "\n",
    "if len(hourly_dow) > 0:\n",
    "    pivot = hourly_dow.pivot_table(index='weekday', columns='hour', values='searches',\n",
    "                                    aggfunc='sum', fill_value=0)\n",
    "    dow_order = hourly_dow.drop_duplicates('weekday').sort_values('dow_num')['weekday'].tolist()\n",
    "    pivot = pivot.reindex(dow_order)\n",
    "    sns.heatmap(pivot, cmap='YlOrRd', ax=ax2, fmt='.0f',\n",
    "                cbar_kws={'label': 'Searches'})\n",
    "    ax2.set_title('Search Heatmap: Day x Hour (CET)')\n",
    "    ax2.set_xlabel('Hour (CET)')\n",
    "    ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Peak hours\n",
    "peak_hours = query(\"\"\"\n",
    "    SELECT event_hour as hour, COUNT(*) as searches\n",
    "    FROM searches WHERE name = 'SEARCH_TRIGGERED'\n",
    "    GROUP BY 1 ORDER BY 2 DESC LIMIT 3\n",
    "\"\"\")\n",
    "print(\"Top 3 peak hours (CET):\")\n",
    "for _, row in peak_hours.iterrows():\n",
    "    print(f\"  {int(row['hour']):02d}:00 -- {int(row['searches']):,} searches\")\n",
    "\n",
    "# Regional percentages\n",
    "total_regional = values.sum()\n",
    "if total_regional > 0:\n",
    "    print(f\"\\nRegional breakdown:\")\n",
    "    for label, val in zip(labels, values):\n",
    "        print(f\"  {label}: {val:,.0f} ({100*val/total_regional:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"Time-of-day analysis confirmed our intranet serves a global workforce. **[X]%** of searches occur during CET business hours (09-16), **[Y]%** during Americas hours (16-22 CET), and **[Z]%** during APAC hours (03-09 CET). The peak search hour is **[HH]:00 CET**. This data informed content publishing schedules -- publishing before the peak ensures content is fresh when most users search. The day-of-week heatmap revealed the exact windows of highest activity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 8. Search Ranking & Performance\n**Business Question:** How effective is our search ranking? Are users finding results at the top, or do they need to scroll? Does search latency affect user behavior?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== CLICK POSITION & SEARCH LATENCY =====\n\n# 1. Click position distribution\nclick_pos = query(\"\"\"\n    SELECT clicked_position, COUNT(*) as clicks\n    FROM searches\n    WHERE click_category = 'Result' AND clicked_position IS NOT NULL\n    GROUP BY clicked_position\n    ORDER BY clicked_position\n\"\"\")\n\n# 2. Daily average click position trend\ndaily_pos = query(\"\"\"\n    SELECT date,\n           sum_click_position,\n           click_position_count,\n           ROUND(1.0 * sum_click_position / NULLIF(click_position_count, 0), 1) as avg_click_position\n    FROM searches_daily\n    WHERE click_position_count > 0\n    ORDER BY date\n\"\"\")\ndaily_pos['date'] = pd.to_datetime(daily_pos['date'])\n\n# 3. Latency vs abandonment\nlatency_impact = query(\"\"\"\n    SELECT\n        CASE\n            WHEN avg_search_latency_ms < 500 THEN '<0.5s'\n            WHEN avg_search_latency_ms < 1000 THEN '0.5-1s'\n            WHEN avg_search_latency_ms < 2000 THEN '1-2s'\n            WHEN avg_search_latency_ms < 5000 THEN '2-5s'\n            ELSE '>5s'\n        END as latency_bucket,\n        MIN(avg_search_latency_ms) as sort_val,\n        COUNT(*) as sessions,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as success_rate,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Abandoned' THEN 1 ELSE 0 END) / COUNT(*), 1) as abandon_rate\n    FROM searches_journeys\n    WHERE avg_search_latency_ms IS NOT NULL\n    GROUP BY 1\n    ORDER BY MIN(avg_search_latency_ms)\n\"\"\")\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Click position distribution\nif len(click_pos) > 0:\n    top_pos = click_pos.head(10)\n    total_clicks = click_pos['clicks'].sum()\n    top_pos = top_pos.copy()\n    top_pos['pct'] = 100.0 * top_pos['clicks'] / total_clicks\n    axes[0].bar(top_pos['clicked_position'].astype(str), top_pos['pct'], color=C['neutral'])\n    axes[0].set_xlabel('Result Position')\n    axes[0].set_ylabel('% of All Clicks')\n    axes[0].set_title('Click Position Distribution')\n    for i, (_, row) in enumerate(top_pos.iterrows()):\n        axes[0].text(i, row['pct'] + 0.3, f\"{row['pct']:.0f}%\", ha='center', fontsize=9)\n\n# Avg click position trend\nif len(daily_pos) > 0:\n    axes[1].plot(daily_pos['date'], daily_pos['avg_click_position'],\n                color=C['neutral'], linewidth=2, marker='o', markersize=3)\n    axes[1].set_ylabel('Avg Click Position')\n    axes[1].set_title('Daily Avg Click Position (lower = better)')\n    axes[1].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n    axes[1].tick_params(axis='x', rotation=45)\n\n# Latency vs outcomes\nif len(latency_impact) > 0:\n    x = range(len(latency_impact))\n    width = 0.35\n    axes[2].bar([i - width/2 for i in x], latency_impact['success_rate'], width,\n               label='Success %', color=C['success'])\n    axes[2].bar([i + width/2 for i in x], latency_impact['abandon_rate'], width,\n               label='Abandoned %', color=C['warn'])\n    axes[2].set_xticks(list(x))\n    axes[2].set_xticklabels(latency_impact['latency_bucket'])\n    axes[2].set_xlabel('Search Latency')\n    axes[2].set_ylabel('Rate %')\n    axes[2].set_title('Latency Impact on User Behavior')\n    axes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Summary stats\nRANKING = {}\nif len(click_pos) > 0:\n    total_clicks = click_pos['clicks'].sum()\n    pos1_clicks = click_pos[click_pos['clicked_position'] == 1]['clicks'].sum()\n    pos1_pct = 100.0 * pos1_clicks / total_clicks if total_clicks > 0 else 0\n    top3_clicks = click_pos[click_pos['clicked_position'] <= 3]['clicks'].sum()\n    top3_pct = 100.0 * top3_clicks / total_clicks if total_clicks > 0 else 0\n    overall_avg_pos = daily_pos['sum_click_position'].sum() / max(daily_pos['click_position_count'].sum(), 1)\n\n    mrr_data = query(\"\"\"\n        SELECT ROUND(AVG(1.0 / clicked_position), 3) as mrr\n        FROM searches\n        WHERE click_category = 'Result' AND clicked_position IS NOT NULL AND clicked_position > 0\n    \"\"\")\n\n    RANKING = {\n        'pos1_pct': pos1_pct, 'top3_pct': top3_pct,\n        'avg_position': overall_avg_pos,\n        'mrr': mrr_data['mrr'][0] if len(mrr_data) > 0 and not pd.isna(mrr_data['mrr'][0]) else None\n    }\n\n    print(f\"Click Position Analysis:\")\n    print(f\"  Position 1 clicks:     {pos1_pct:.1f}% of all result clicks\")\n    print(f\"  Top 3 positions:       {top3_pct:.1f}% of all result clicks\")\n    print(f\"  Overall avg position:  {overall_avg_pos:.1f}\")\n    if RANKING['mrr'] is not None:\n        print(f\"  Mean Reciprocal Rank:  {RANKING['mrr']:.3f} (1.0 = perfect)\")\n\nif len(latency_impact) > 0:\n    print(f\"\\nLatency Impact:\")\n    for _, row in latency_impact.iterrows():\n        print(f\"  {row['latency_bucket']:<10s} {int(row['sessions']):>6,} sessions | \"\n              f\"Success: {row['success_rate']:.1f}% | Abandoned: {row['abandon_rate']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaway (Interview-Ready)\n> \"I measured search ranking effectiveness using click position analysis. **[X]%** of clicks land on position 1 and **[Y]%** on the top 3 results, with a Mean Reciprocal Rank of **[MRR]**. I also correlated search latency with user behavior: sessions with latency above 2 seconds showed significantly higher abandonment rates, providing data to support infrastructure investment. This is not just a content problem -- it's a technical quality problem that affects user experience.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 9. Audience Intelligence\n**Business Question:** Which departments and user segments are we serving well? Where are the gaps? How does search behavior differ across languages and devices?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== DEPARTMENT, LANGUAGE & DEVICE ANALYSIS =====\n\n# 1. Department analysis\ndept = query(\"\"\"\n    SELECT\n        department,\n        COUNT(*) as sessions,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as success_rate,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='No Results' THEN 1 ELSE 0 END) / COUNT(*), 1) as no_result_rate,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Abandoned' THEN 1 ELSE 0 END) / COUNT(*), 1) as abandon_rate,\n        ROUND(100.0 * SUM(CASE WHEN had_reformulation THEN 1 ELSE 0 END) / COUNT(*), 1) as reformulation_rate\n    FROM searches_journeys\n    WHERE department IS NOT NULL AND department != ''\n    GROUP BY department\n    HAVING COUNT(*) >= 3\n    ORDER BY sessions DESC\n\"\"\")\n\n# 2. Language analysis\nlang = query(\"\"\"\n    SELECT\n        query_language,\n        COUNT(*) as sessions,\n        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct_of_total,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as success_rate,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='No Results' THEN 1 ELSE 0 END) / COUNT(*), 1) as no_result_rate\n    FROM searches_journeys\n    WHERE query_language IS NOT NULL AND query_language != ''\n    GROUP BY query_language\n    ORDER BY sessions DESC\n\"\"\")\n\n# 3. Device analysis\ndevice = query(\"\"\"\n    SELECT\n        device_type,\n        COUNT(*) as sessions,\n        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct_of_total,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as success_rate,\n        ROUND(100.0 * SUM(CASE WHEN journey_outcome='Abandoned' THEN 1 ELSE 0 END) / COUNT(*), 1) as abandon_rate,\n        ROUND(AVG(CASE WHEN sec_result_to_click IS NOT NULL THEN sec_result_to_click END), 1) as avg_time_to_click\n    FROM searches_journeys\n    WHERE device_type IS NOT NULL AND device_type != ''\n    GROUP BY device_type\n    ORDER BY sessions DESC\n\"\"\")\n\n# Visualize\nn_charts = sum([len(dept) > 0, len(lang) > 0, len(device) > 0])\nif n_charts > 0:\n    fig, axes = plt.subplots(1, max(n_charts, 1), figsize=(6 * max(n_charts, 1), 6))\n    if n_charts == 1:\n        axes = [axes]\n    ax_idx = 0\n\n    if len(dept) > 0:\n        top_dept = dept.head(10).sort_values('sessions')\n        axes[ax_idx].barh(top_dept['department'], top_dept['sessions'], color=C['neutral'])\n        for i, (_, row) in enumerate(top_dept.iterrows()):\n            axes[ax_idx].text(row['sessions'] + max(top_dept['sessions'])*0.02, i,\n                    f\"Success: {row['success_rate']:.0f}%\", va='center', fontsize=9)\n        axes[ax_idx].set_xlabel('Sessions')\n        axes[ax_idx].set_title('Top Departments by Search Volume')\n        ax_idx += 1\n\n    if len(lang) > 0:\n        top_lang = lang.head(8)\n        axes[ax_idx].bar(top_lang['query_language'], top_lang['sessions'], color=C['neutral'])\n        axes[ax_idx].set_ylabel('Sessions')\n        axes[ax_idx].set_title('Search Sessions by Language')\n        axes[ax_idx].tick_params(axis='x', rotation=45)\n        for i, (_, row) in enumerate(top_lang.iterrows()):\n            axes[ax_idx].text(i, row['sessions'], f\"{row['pct_of_total']:.0f}%\",\n                    ha='center', va='bottom', fontsize=9)\n        ax_idx += 1\n\n    if len(device) > 0:\n        dev_colors = [C['neutral'] if i == 0 else C['neutral_light'] for i in range(len(device))]\n        axes[ax_idx].bar(device['device_type'], device['sessions'], color=dev_colors)\n        axes[ax_idx].set_ylabel('Sessions')\n        axes[ax_idx].set_title('Sessions by Device Type')\n        for i, (_, row) in enumerate(device.iterrows()):\n            axes[ax_idx].text(i, row['sessions'], f\"{row['pct_of_total']:.0f}%\",\n                    ha='center', va='bottom', fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\n# Tables\nif len(dept) > 0:\n    print(\"DEPARTMENT SEARCH BEHAVIOR (min 3 sessions)\")\n    display(dept.head(15).rename(columns={\n        'department': 'Department', 'sessions': 'Sessions',\n        'success_rate': 'Success %', 'no_result_rate': 'No Result %',\n        'abandon_rate': 'Abandoned %', 'reformulation_rate': 'Reformulation %'\n    }).style.hide(axis='index')\n    .format({'Sessions': '{:,}', 'Success %': '{:.1f}%', 'No Result %': '{:.1f}%',\n             'Abandoned %': '{:.1f}%', 'Reformulation %': '{:.1f}%'})\n    .bar(subset=['Success %'], color=C['success_light'], vmin=0, vmax=100))\n\nif len(lang) > 0:\n    print(\"\\nLANGUAGE ANALYSIS\")\n    display(lang.rename(columns={\n        'query_language': 'Language', 'sessions': 'Sessions', 'pct_of_total': '% of Total',\n        'success_rate': 'Success %', 'no_result_rate': 'No Result %'\n    }).style.hide(axis='index')\n    .format({'Sessions': '{:,}', '% of Total': '{:.1f}%',\n             'Success %': '{:.1f}%', 'No Result %': '{:.1f}%'}))\n\nif len(device) > 0:\n    print(\"\\nDEVICE TYPE ANALYSIS\")\n    display(device.rename(columns={\n        'device_type': 'Device', 'sessions': 'Sessions', 'pct_of_total': '% of Total',\n        'success_rate': 'Success %', 'abandon_rate': 'Abandoned %',\n        'avg_time_to_click': 'Avg Secs to Click'\n    }).style.hide(axis='index')\n    .format({'Sessions': '{:,}', '% of Total': '{:.1f}%',\n             'Success %': '{:.1f}%', 'Abandoned %': '{:.1f}%', 'Avg Secs to Click': '{:.1f}'}))\n\n# Summary insights\nAUDIENCE = {}\nif len(dept) > 0:\n    best_dept = dept.loc[dept['success_rate'].idxmax()]\n    worst_dept = dept.loc[dept['success_rate'].idxmin()]\n    AUDIENCE['n_departments'] = len(dept)\n    AUDIENCE['top_dept'] = dept.iloc[0]['department']\n    AUDIENCE['top_dept_sessions'] = int(dept.iloc[0]['sessions'])\n    AUDIENCE['best_dept'] = best_dept['department']\n    AUDIENCE['best_dept_success'] = best_dept['success_rate']\n    AUDIENCE['worst_dept'] = worst_dept['department']\n    AUDIENCE['worst_dept_success'] = worst_dept['success_rate']\n    gap = best_dept['success_rate'] - worst_dept['success_rate']\n    print(f\"\\nDepartment insights:\")\n    print(f\"  {len(dept)} departments | Highest volume: {dept.iloc[0]['department']} ({int(dept.iloc[0]['sessions']):,} sessions)\")\n    print(f\"  Best success rate:   {best_dept['department']} ({best_dept['success_rate']:.1f}%)\")\n    print(f\"  Lowest success rate: {worst_dept['department']} ({worst_dept['success_rate']:.1f}%)\")\n    print(f\"  Gap: {gap:.1f} percentage points\")\n\nif len(lang) > 0:\n    primary = lang.iloc[0]\n    non_primary_pct = 100.0 - primary['pct_of_total'] if len(lang) > 1 else 0\n    AUDIENCE['primary_language'] = primary['query_language']\n    AUDIENCE['primary_language_pct'] = primary['pct_of_total']\n    AUDIENCE['non_primary_pct'] = non_primary_pct\n    AUDIENCE['n_languages'] = len(lang)\n    print(f\"\\nLanguage insights:\")\n    print(f\"  Primary: {primary['query_language']} ({primary['pct_of_total']:.1f}%)\")\n    if len(lang) > 1:\n        print(f\"  {len(lang) - 1} other languages ({non_primary_pct:.1f}% of sessions)\")\n\nif len(device) > 0:\n    AUDIENCE['top_device'] = device.iloc[0]['device_type']\n    AUDIENCE['top_device_pct'] = device.iloc[0]['pct_of_total']\n    print(f\"\\nDevice insights:\")\n    for _, row in device.iterrows():\n        print(f\"  {row['device_type']}: {row['pct_of_total']:.1f}% | Success: {row['success_rate']:.1f}% | Abandoned: {row['abandon_rate']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaway (Interview-Ready)\n> \"I segmented search behavior by department, language, and device type to identify underserved audiences. **[N]** departments were active, with a **[gap]** percentage point gap in success rates between the best and worst performers. **[X]%** of searches were in non-primary languages with higher failure rates -- providing the business case for multilingual content investment. Device analysis showed **[device]** users accounted for **[Y]%** of sessions. This audience segmentation turned generic 'search isn't working' complaints into specific, actionable improvement plans per audience.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 10. Content & Navigation\n**Business Question:** What content do users actually click on? How do they use search navigation features like tabs and filters?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ===== MOST-CLICKED CONTENT & NAVIGATION FEATURES =====\n\n# 1. Most-clicked content\ntop_content = query(\"\"\"\n    SELECT\n        clicked_result_title as title,\n        clicked_result_url as url,\n        COUNT(*) as clicks,\n        COUNT(DISTINCT user_id) as unique_users,\n        COUNT(DISTINCT session_key) as unique_sessions\n    FROM searches\n    WHERE is_success_click = true\n      AND clicked_result_title IS NOT NULL AND clicked_result_title != ''\n    GROUP BY clicked_result_title, clicked_result_url\n    ORDER BY clicks DESC\n    LIMIT 20\n\"\"\")\n\n# 2. Tab & Filter adoption\nnav_adoption = query(\"\"\"\n    SELECT\n        COUNT(*) as total_sessions,\n        SUM(CASE WHEN distinct_tabs_clicked > 0 THEN 1 ELSE 0 END) as tab_users,\n        SUM(CASE WHEN distinct_filters_used > 0 THEN 1 ELSE 0 END) as filter_users,\n        ROUND(100.0 * SUM(CASE WHEN distinct_tabs_clicked > 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_using_tabs,\n        ROUND(100.0 * SUM(CASE WHEN distinct_filters_used > 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_using_filters,\n        ROUND(100.0 * SUM(CASE WHEN distinct_tabs_clicked > 0 AND journey_outcome='Success' THEN 1 ELSE 0 END)\n            / NULLIF(SUM(CASE WHEN distinct_tabs_clicked > 0 THEN 1 ELSE 0 END), 0), 1) as tab_success_rate,\n        ROUND(100.0 * SUM(CASE WHEN distinct_tabs_clicked = 0 AND journey_outcome='Success' THEN 1 ELSE 0 END)\n            / NULLIF(SUM(CASE WHEN distinct_tabs_clicked = 0 THEN 1 ELSE 0 END), 0), 1) as no_tab_success_rate,\n        ROUND(100.0 * SUM(CASE WHEN distinct_filters_used > 0 AND journey_outcome='Success' THEN 1 ELSE 0 END)\n            / NULLIF(SUM(CASE WHEN distinct_filters_used > 0 THEN 1 ELSE 0 END), 0), 1) as filter_success_rate,\n        ROUND(100.0 * SUM(CASE WHEN distinct_filters_used = 0 AND journey_outcome='Success' THEN 1 ELSE 0 END)\n            / NULLIF(SUM(CASE WHEN distinct_filters_used = 0 THEN 1 ELSE 0 END), 0), 1) as no_filter_success_rate\n    FROM searches_journeys\n\"\"\")\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Top content bar chart\nif len(top_content) > 0:\n    top_n = top_content.head(10).sort_values('clicks')\n    top_n = top_n.copy()\n    top_n['short_title'] = top_n['title'].str[:50] + np.where(top_n['title'].str.len() > 50, '...', '')\n    axes[0].barh(top_n['short_title'], top_n['clicks'], color=C['success'])\n    for i, (_, row) in enumerate(top_n.iterrows()):\n        axes[0].text(row['clicks'] + max(top_n['clicks'])*0.02, i,\n                f\"{int(row['clicks']):,} ({int(row['unique_users'])} users)\",\n                va='center', fontsize=9)\n    axes[0].set_xlabel('Total Clicks')\n    axes[0].set_title('Top 10 Most-Clicked Content from Search')\nelse:\n    axes[0].text(0.5, 0.5, 'No click data available', transform=axes[0].transAxes,\n                ha='center', va='center', fontsize=12, color=C['gray'])\n    axes[0].set_title('Most-Clicked Content')\n    axes[0].axis('off')\n\n# Tab & Filter adoption\nif len(nav_adoption) > 0:\n    n = nav_adoption.iloc[0]\n    features = ['Tabs', 'Filters']\n    adoption = [n['pct_using_tabs'], n['pct_using_filters']]\n    with_success = [n['tab_success_rate'], n['filter_success_rate']]\n    without_success = [n['no_tab_success_rate'], n['no_filter_success_rate']]\n\n    x = range(len(features))\n    width = 0.25\n    axes[1].bar([i - width for i in x], adoption, width, label='Adoption %', color=C['neutral'])\n    axes[1].bar([i for i in x], with_success, width, label='Success % (users)', color=C['success'])\n    axes[1].bar([i + width for i in x], without_success, width, label='Success % (non-users)', color=C['gray'])\n    axes[1].set_xticks(list(x))\n    axes[1].set_xticklabels(features)\n    axes[1].set_ylabel('Rate %')\n    axes[1].set_title('Search Feature Adoption & Impact')\n    axes[1].legend()\n    for i, v in enumerate(adoption):\n        axes[1].text(i - width, v + 0.5, f'{v:.0f}%', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# Tables\nif len(top_content) > 0:\n    print(\"TOP 20 MOST-CLICKED CONTENT FROM SEARCH\")\n    print(\"These pages are what employees actually find useful via search.\\n\")\n    display(top_content[['title', 'clicks', 'unique_users', 'unique_sessions']].rename(columns={\n        'title': 'Page Title', 'clicks': 'Clicks',\n        'unique_users': 'Unique Users', 'unique_sessions': 'Sessions'\n    }).style.hide(axis='index')\n    .format({'Clicks': '{:,}', 'Unique Users': '{:,}', 'Sessions': '{:,}'}))\n\nCONTENT = {}\nif len(nav_adoption) > 0:\n    n = nav_adoption.iloc[0]\n    CONTENT['tab_adoption'] = n['pct_using_tabs']\n    CONTENT['filter_adoption'] = n['pct_using_filters']\n    CONTENT['tab_success'] = n['tab_success_rate']\n    CONTENT['no_tab_success'] = n['no_tab_success_rate']\n    CONTENT['filter_success'] = n['filter_success_rate']\n    CONTENT['no_filter_success'] = n['no_filter_success_rate']\n    print(f\"\\nSEARCH FEATURE ADOPTION\")\n    print(f\"  Tab switching:  {n['pct_using_tabs']:.1f}% of sessions ({int(n['tab_users']):,} sessions)\")\n    print(f\"    Success rate WITH tabs:    {n['tab_success_rate']:.1f}%\")\n    print(f\"    Success rate WITHOUT tabs:  {n['no_tab_success_rate']:.1f}%\")\n    print(f\"  Filter usage:   {n['pct_using_filters']:.1f}% of sessions ({int(n['filter_users']):,} sessions)\")\n    print(f\"    Success rate WITH filters:    {n['filter_success_rate']:.1f}%\")\n    print(f\"    Success rate WITHOUT filters:  {n['no_filter_success_rate']:.1f}%\")\n\nif len(top_content) > 0:\n    CONTENT['top_page'] = top_content.iloc[0]['title']\n    CONTENT['top_page_clicks'] = int(top_content.iloc[0]['clicks'])\n    CONTENT['top_page_users'] = int(top_content.iloc[0]['unique_users'])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaway (Interview-Ready)\n> \"I analyzed what content employees actually click on after searching, creating a 'greatest hits' list of the top 20 pages. The #1 result, '[title]', was clicked **[N]** times by **[M]** users -- these pages deserve the most editorial attention. I also measured search feature adoption: only **[X]%** of sessions used tabs and **[Y]%** used filters. Users who did use these features had a **[Z]%** success rate vs **[W]%** for those who didn't, suggesting either the features are undiscoverable or that power users naturally find more.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. Content Discovery: What Do Users Actually Click?\n",
    "\n",
    "Maps search terms to the content users clicked \u2014 bridging search intent to content discovery."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ===== TOP SEARCH TERM \u2192 CLICKED CONTENT PAIRS =====\n",
    "# What content do users find when they search for specific terms?\n",
    "term_clicks = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term,\n",
    "        clicked_result_title,\n",
    "        clicked_result_url,\n",
    "        SUM(click_count) as total_clicks,\n",
    "        SUM(unique_users) as total_users,\n",
    "        SUM(unique_sessions) as total_sessions,\n",
    "        ROUND(SUM(sum_click_position) * 1.0 / NULLIF(SUM(click_position_count), 0), 1) as avg_click_position\n",
    "    FROM searches_term_clicks\n",
    "    GROUP BY search_term, clicked_result_title, clicked_result_url\n",
    "    ORDER BY total_clicks DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP 20 SEARCH TERM \u2192 CONTENT PAIRS\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in term_clicks.iterrows():\n",
    "    pos = f\" (pos {row['avg_click_position']})\" if row['avg_click_position'] else \"\"\n",
    "    print(f\"  '{row['search_term']}' \u2192 {row['clicked_result_title']}: \"\n",
    "          f\"{row['total_clicks']} clicks by {row['total_users']} users{pos}\")\n",
    "\n",
    "# Most clicked content items (regardless of search term)\n",
    "top_content = query(\"\"\"\n",
    "    SELECT\n",
    "        clicked_result_title,\n",
    "        clicked_result_url,\n",
    "        COUNT(DISTINCT search_term) as terms_leading_here,\n",
    "        SUM(click_count) as total_clicks,\n",
    "        SUM(unique_users) as total_users,\n",
    "        ROUND(SUM(sum_click_position) * 1.0 / NULLIF(SUM(click_position_count), 0), 1) as avg_click_position\n",
    "    FROM searches_term_clicks\n",
    "    GROUP BY clicked_result_title, clicked_result_url\n",
    "    ORDER BY total_clicks DESC\n",
    "    LIMIT 15\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"TOP 15 MOST-DISCOVERED CONTENT ITEMS\")\n",
    "print(\"=\" * 70)\n",
    "for _, row in top_content.iterrows():\n",
    "    print(f\"  {row['clicked_result_title']}\")\n",
    "    print(f\"    {row['total_clicks']} clicks from {row['terms_leading_here']} different search terms, \"\n",
    "          f\"{row['total_users']} users, avg position {row['avg_click_position']}\")\n",
    "\n",
    "# Content scatter: terms with many different URLs clicked (scattered content)\n",
    "scattered = query(\"\"\"\n",
    "    SELECT\n",
    "        search_term,\n",
    "        COUNT(DISTINCT clicked_result_url) as distinct_urls,\n",
    "        SUM(click_count) as total_clicks\n",
    "    FROM searches_term_clicks\n",
    "    GROUP BY search_term\n",
    "    HAVING COUNT(DISTINCT clicked_result_url) >= 3\n",
    "    ORDER BY distinct_urls DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "if not scattered.empty:\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"SCATTERED CONTENT: Terms leading to 3+ different URLs\")\n",
    "    print(\"(Consolidation opportunity \u2014 users click many different pages)\")\n",
    "    print(\"=\" * 70)\n",
    "    for _, row in scattered.iterrows():\n",
    "        print(f\"  '{row['search_term']}': {row['distinct_urls']} different URLs clicked ({row['total_clicks']} total clicks)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway (Interview-Ready)\n",
    "> \"By mapping search terms to clicked content, I identified which intranet pages actually satisfy user searches. \"\n",
    "> \"This revealed **content consolidation opportunities** where users clicked 3+ different pages for the same query, \"\n",
    "> \"and **content discovery patterns** showing which pages serve as search landing pages across the organization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 12. Interview Cheat Sheet\n",
    "All key numbers compiled in one place for quick reference during STAR-based interview answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== COMPILE ALL KEY NUMBERS =====\ncheat = query(\"\"\"\n    WITH daily_totals AS (\n        SELECT\n            MIN(date) as date_from, MAX(date) as date_to,\n            COUNT(*) as days,\n            SUM(search_starts) as total_searches,\n            SUM(unique_users) as total_user_days,\n            SUM(unique_sessions) as total_sessions,\n            ROUND(100.0 * SUM(sessions_with_clicks) / NULLIF(SUM(sessions_with_results), 0), 1) as session_success_rate,\n            ROUND(100.0 * SUM(null_results) / NULLIF(SUM(result_events), 0), 1) as null_result_rate,\n            ROUND(100.0 * SUM(sessions_abandoned) / NULLIF(SUM(sessions_with_results), 0), 1) as abandonment_rate,\n            ROUND(SUM(search_starts) * 1.0 / NULLIF(SUM(unique_sessions), 0), 2) as avg_searches_per_session,\n            SUM(null_results) as total_null_results,\n            SUM(new_users) as total_new_users,\n            SUM(returning_users) as total_returning_users,\n            -- Click position (building blocks)\n            ROUND(1.0 * SUM(sum_click_position) / NULLIF(SUM(click_position_count), 0), 1) as avg_click_position,\n            SUM(click_position_count) as total_result_clicks,\n            -- Latency (building blocks)\n            ROUND(1.0 * SUM(sum_search_latency_ms) / NULLIF(SUM(latency_event_count), 0), 0) as avg_latency_ms\n        FROM searches_daily\n    ),\n    journey_totals AS (\n        SELECT\n            COUNT(*) as total_journeys,\n            ROUND(100.0 * SUM(CASE WHEN had_reformulation THEN 1 ELSE 0 END) / COUNT(*), 1) as reformulation_rate,\n            ROUND(100.0 * SUM(CASE WHEN recovered_from_null THEN 1 ELSE 0 END)\n                / NULLIF(SUM(CASE WHEN had_null_result THEN 1 ELSE 0 END), 0), 1) as null_recovery_rate,\n            ROUND(AVG(CASE WHEN sec_result_to_click IS NOT NULL THEN sec_result_to_click END), 1) as avg_sec_to_click,\n            ROUND(AVG(CASE WHEN sec_search_to_result IS NOT NULL THEN sec_search_to_result END), 2) as avg_latency,\n            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Success' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_success,\n            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Abandoned' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_abandoned,\n            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'No Results' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_no_results,\n            ROUND(100.0 * SUM(CASE WHEN journey_outcome = 'Engaged' THEN 1 ELSE 0 END) / COUNT(*), 1) as pct_engaged,\n            -- Audience metrics\n            COUNT(DISTINCT department) as n_departments,\n            COUNT(DISTINCT query_language) as n_languages,\n            COUNT(DISTINCT device_type) as n_device_types,\n            -- Feature adoption\n            ROUND(100.0 * SUM(CASE WHEN distinct_tabs_clicked > 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as tab_adoption,\n            ROUND(100.0 * SUM(CASE WHEN distinct_filters_used > 0 THEN 1 ELSE 0 END) / COUNT(*), 1) as filter_adoption\n        FROM searches_journeys\n    ),\n    term_totals AS (\n        SELECT\n            COUNT(DISTINCT search_term) as unique_terms,\n            (SELECT COUNT(*) FROM (\n                SELECT search_term FROM searches_terms GROUP BY search_term\n                HAVING SUM(null_result_count) = SUM(result_events) AND SUM(result_events) > 0\n            )) as pure_zero_result_terms\n        FROM searches_terms\n    ),\n    ranking_totals AS (\n        SELECT\n            ROUND(100.0 * SUM(CASE WHEN clicked_position = 1 THEN 1 ELSE 0 END)\n                / NULLIF(COUNT(*), 0), 1) as pos1_click_pct,\n            ROUND(100.0 * SUM(CASE WHEN clicked_position <= 3 THEN 1 ELSE 0 END)\n                / NULLIF(COUNT(*), 0), 1) as top3_click_pct,\n            ROUND(AVG(1.0 / clicked_position), 3) as mrr\n        FROM searches\n        WHERE click_category = 'Result' AND clicked_position IS NOT NULL AND clicked_position > 0\n    )\n    SELECT * FROM daily_totals, journey_totals, term_totals, ranking_totals\n\"\"\")\n\nc = cheat.iloc[0]\n\ncheat_sheet = pd.DataFrame([\n    ['SCOPE', ''],\n    ['  Period', f\"{c['date_from']} to {c['date_to']} ({int(c['days'])} days)\"],\n    ['  Total Searches', fmt_num(c['total_searches'])],\n    ['  User-Days', fmt_num(c['total_user_days'])],\n    ['  Total Sessions', fmt_num(c['total_sessions'])],\n    ['  Unique Search Terms', fmt_num(c['unique_terms'])],\n    ['', ''],\n    ['QUALITY METRICS', ''],\n    ['  Session Success Rate', fmt_pct(c['session_success_rate'])],\n    ['  Null Result Rate', fmt_pct(c['null_result_rate'])],\n    ['  Abandonment Rate', fmt_pct(c['abandonment_rate'])],\n    ['  Avg Searches/Session', f\"{c['avg_searches_per_session']:.2f}\"],\n    ['  Reformulation Rate', fmt_pct(c['reformulation_rate'])],\n    ['  Null Recovery Rate', fmt_pct(c['null_recovery_rate'])],\n    ['', ''],\n    ['JOURNEY OUTCOMES', ''],\n    ['  Success (clicked result)', fmt_pct(c['pct_success'])],\n    ['  Engaged (navigated, no click)', fmt_pct(c['pct_engaged'])],\n    ['  Abandoned (results, no action)', fmt_pct(c['pct_abandoned'])],\n    ['  No Results', fmt_pct(c['pct_no_results'])],\n    ['', ''],\n    ['RANKING & PERFORMANCE', ''],\n    ['  Avg Click Position', f\"{c['avg_click_position']:.1f}\" if not pd.isna(c['avg_click_position']) else 'N/A'],\n    ['  Position 1 Click %', fmt_pct(c['pos1_click_pct'])],\n    ['  Top 3 Click %', fmt_pct(c['top3_click_pct'])],\n    ['  Mean Reciprocal Rank (MRR)', f\"{c['mrr']:.3f}\" if not pd.isna(c['mrr']) else 'N/A'],\n    ['  Avg Search Latency', f\"{c['avg_latency_ms']:.0f}ms\" if not pd.isna(c['avg_latency_ms']) else 'N/A'],\n    ['  Avg Time to Click', f\"{c['avg_sec_to_click']:.1f}s\"],\n    ['', ''],\n    ['AUDIENCE', ''],\n    ['  Departments', fmt_num(c['n_departments'])],\n    ['  Languages', fmt_num(c['n_languages'])],\n    ['  Device Types', fmt_num(c['n_device_types'])],\n    ['  Tab Adoption', fmt_pct(c['tab_adoption'])],\n    ['  Filter Adoption', fmt_pct(c['filter_adoption'])],\n    ['', ''],\n    ['CONTENT GAPS', ''],\n    ['  Total Failed Searches', fmt_num(c['total_null_results'])],\n    ['  Terms Always Returning Zero', fmt_num(c['pure_zero_result_terms'])],\n    ['', ''],\n    ['USER ADOPTION', ''],\n    ['  New Users', f\"{fmt_num(c['total_new_users'])} ({100*c['total_new_users']/(c['total_new_users']+c['total_returning_users']):.0f}%)\"],\n    ['  Returning Users', f\"{fmt_num(c['total_returning_users'])} ({100*c['total_returning_users']/(c['total_new_users']+c['total_returning_users']):.0f}%)\"],\n], columns=['Metric', 'Value'])\n\nprint(\"=\" * 56)\nprint(\"  INTERVIEW REFERENCE: ALL KEY NUMBERS\")\nprint(\"=\" * 56)\nfor _, row in cheat_sheet.iterrows():\n    if row['Value'] == '':\n        print(f\"\\n  {row['Metric']}\")\n    else:\n        print(f\"  {row['Metric']:<40s} {row['Value']}\")\nprint(\"\\n\" + \"=\" * 56)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAR Talking Points\n\nFill in the numbers from above and rehearse these stories:\n\n---\n\n#### Story 1: Building the Analytics Pipeline (Technical Leadership)\n- **S:** Our intranet search had no analytics -- zero visibility into what employees were searching for or whether they found content.\n- **T:** I was tasked with designing and building an end-to-end search analytics solution from raw telemetry to executive dashboards.\n- **A:** I built a Python ETL pipeline (1,000+ lines) that ingests Azure App Insights telemetry, enriches it with session analysis and journey classification, and outputs analytics-ready data. I designed 30+ DAX measures for Power BI and created 5 dashboard pages. I documented the complete data model and visualization guide.\n- **R:** We went from zero visibility to tracking **[total_searches]** searches across **[users]** users, with automated session classification, content gap detection, and quality scorecards. The solution now runs weekly and serves senior management.\n\n---\n\n#### Story 2: Identifying Content Gaps (Business Impact)\n- **S:** Internal Communications had no data-driven way to decide what intranet content to create or improve.\n- **T:** Use search analytics to identify the most impactful content gaps.\n- **A:** I analyzed zero-result search terms and identified **[pure_zero_result_terms]** unique terms that consistently return no results, ranked by volume.\n- **R:** This gave Internal Comms a prioritized content roadmap. Addressing just the top 10 gaps would cover **[X]%** of all gap-related searches. Each term on the list represents a direct, measurable improvement opportunity.\n\n---\n\n#### Story 3: Measuring Search Quality (Data-Driven Decisions)\n- **S:** Leadership needed to know if the intranet was actually working -- no baseline metrics existed.\n- **T:** Define and implement quality KPIs that an executive audience would understand and act on.\n- **A:** I designed a scorecard with Session Success Rate (**[X]%**), Null Result Rate (**[Y]%**), and Abandonment Rate (**[Z]%**). I set benchmark targets based on industry standards and trended these weekly.\n- **R:** For the first time, leadership could see a single number (session success rate) and understand whether search was improving. The reformulation rate of **[W]%** became a key input for the search team's optimization roadmap.\n\n---\n\n#### Story 4: Global Workforce Insights (Strategic Thinking)\n- **S:** We assumed our intranet served primarily a European audience but had no data.\n- **T:** Analyze temporal patterns to understand global usage.\n- **A:** I implemented CET-based time zone analysis, classifying searches into APAC, CET, and Americas windows.\n- **R:** Data revealed the actual regional split, informing content publishing schedules and multilingual content priorities. Publishing before peak hour ensures maximum reach.\n\n---\n\n#### Story 5: Search Ranking & Performance (Technical Depth)\n- **S:** We knew content gaps existed, but we had no way to distinguish \"content doesn't exist\" from \"content exists but search ranks it poorly.\"\n- **T:** Build ranking effectiveness metrics to separate content problems from search relevance problems.\n- **A:** I implemented click position tracking and calculated Mean Reciprocal Rank (MRR). I found that **[X]%** of clicks landed on position 1 and **[Y]%** on the top 3 results. I also correlated search latency with user behavior, showing that sessions with higher latency had significantly more abandonment.\n- **R:** This distinguished two categories of improvement: content gaps (create new content) vs. ranking problems (improve metadata/relevance for existing content). The latency analysis provided data supporting infrastructure investment.\n\n---\n\n#### Story 6: Audience Segmentation (Stakeholder Value)\n- **S:** \"Search isn't working\" was a generic complaint with no specifics about *who* it wasn't working for.\n- **T:** Segment search behavior by department, language, and device to identify underserved audiences.\n- **A:** I analyzed search outcomes across **[N]** departments, **[N]** languages, and **[N]** device types. I found a **[gap]** percentage point gap in success rates between the best and worst performing departments. Non-primary language searches had higher failure rates.\n- **R:** Each department now had a specific search quality score. The worst-performing department became the focus of a targeted content improvement initiative. The language analysis justified investment in multilingual content.\n\n\n\n---\n\n#### Story 8: Content Discovery Mapping (Data-Driven Content Strategy)\n- **S:** We could see *what* users searched but not *what they actually found and clicked*\n- **T:** Map search terms to clicked content to understand content discovery pathways\n- **A:** Built a search term \u2192 clicked content pipeline revealing which pages satisfy which queries. Identified scattered content (3+ URLs clicked for same query) as consolidation opportunities\n- **R:** Enabled content team to see \"when users search for [X], they click [Y]\" \u2014 identified **[N] content consolidation opportunities** and **top [N] content landing pages** from search\n\n---\n\n#### Story 7: Content Intelligence (Actionable Insights)\n- **S:** Internal Communications was publishing content based on editorial judgment, not data on what employees actually needed.\n- **T:** Identify the most valuable intranet content as measured by actual employee search behavior.\n- **A:** I built a \"greatest hits\" analysis showing the top 20 most-clicked pages from search, with click counts and unique user counts. I also measured search feature adoption (tabs: **[X]%**, filters: **[Y]%**) and correlated it with success rates.\n- **R:** The top-clicked page, \"[title]\", was accessed **[N]** times by **[M]** users via search -- these pages deserve priority editorial attention. The low feature adoption rate informed UX redesign priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CLEANUP =====\n",
    "con.close()\n",
    "print(\"Database connection closed. Notebook complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}